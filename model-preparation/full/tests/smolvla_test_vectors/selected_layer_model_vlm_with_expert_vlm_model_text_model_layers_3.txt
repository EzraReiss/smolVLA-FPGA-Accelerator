├───┬ input_layernorm
|   ├─ in: model.vlm_with_expert.vlm.model.text_model.layers.3.input_layernorm
|   ├─ children: ['weight']
│   └──── weight: torch.Size([960]) [torch.bfloat16]
├───┬ mlp
|   ├─ in: model.vlm_with_expert.vlm.model.text_model.layers.3.mlp
|   ├─ children: ['down_proj', 'gate_proj', 'up_proj']
│   ├───┬ down_proj
│   |   ├─ in: model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.down_proj
│   |   ├─ children: ['weight']
│   │   └──── weight: torch.Size([960, 2560]) [torch.bfloat16]
│   ├───┬ gate_proj
│   |   ├─ in: model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.gate_proj
│   |   ├─ children: ['weight']
│   │   └──── weight: torch.Size([2560, 960]) [torch.bfloat16]
│   └───┬ up_proj:
│        ├─ in: model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.up_proj
│       ├─ children: ['weight']
│       └──── weight: torch.Size([2560, 960]) [torch.bfloat16]
├───┬ post_attention_layernorm
|   ├─ in: model.vlm_with_expert.vlm.model.text_model.layers.3.post_attention_layernorm
|   ├─ children: ['weight']
│   └──── weight: torch.Size([960]) [torch.bfloat16]
└───┬ self_attn:
     ├─ in: model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn
    ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
    ├───┬ k_proj
    |   ├─ in: model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.k_proj
    |   ├─ children: ['weight']
    │   └──── weight: torch.Size([320, 960]) [torch.bfloat16]
    ├───┬ o_proj
    |   ├─ in: model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.o_proj
    |   ├─ children: ['weight']
    │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]
    ├───┬ q_proj
    |   ├─ in: model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.q_proj
    |   ├─ children: ['weight']
    │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]
    └───┬ v_proj:
         ├─ in: model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.v_proj
        ├─ children: ['weight']
        └──── weight: torch.Size([320, 960]) [torch.bfloat16]
