└──┬ model:
   ├─ children: ['action_in_proj', 'action_out_proj', 'action_time_mlp_in', 'action_time_mlp_out', 'state_proj', 'vlm_with_expert']
   ├──┬ action_in_proj
   |  ├─ in: .model.action_in_proj
   |  ├─ children: ['bias', 'weight']
   │  ├── bias: torch.Size([720]) [torch.float32]
   │  └── weight: torch.Size([720, 32]) [torch.float32]
   ├──┬ action_out_proj
   |  ├─ in: .model.action_out_proj
   |  ├─ children: ['bias', 'weight']
   │  ├── bias: torch.Size([32]) [torch.float32]
   │  └── weight: torch.Size([32, 720]) [torch.float32]
   ├──┬ action_time_mlp_in
   |  ├─ in: .model.action_time_mlp_in
   |  ├─ children: ['bias', 'weight']
   │  ├── bias: torch.Size([720]) [torch.float32]
   │  └── weight: torch.Size([720, 1440]) [torch.float32]
   ├──┬ action_time_mlp_out
   |  ├─ in: .model.action_time_mlp_out
   |  ├─ children: ['bias', 'weight']
   │  ├── bias: torch.Size([720]) [torch.float32]
   │  └── weight: torch.Size([720, 720]) [torch.float32]
   ├──┬ state_proj
   |  ├─ in: .model.state_proj
   |  ├─ children: ['bias', 'weight']
   │  ├── bias: torch.Size([960]) [torch.float32]
   │  └── weight: torch.Size([960, 32]) [torch.float32]
   └──┬ vlm_with_expert:
      ├─ in: .model.vlm_with_expert
      ├─ children: ['lm_expert', 'vlm']
      ├──┬ lm_expert
      |  ├─ in: .model.vlm_with_expert.lm_expert
      |  ├─ children: ['layers', 'norm']
      │  ├──┬ layers
      │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers
      │  |  ├─ children: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15']
      │  │  └──┬ [0..15]:
      │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15]
      │  │     ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
      │  │     ├──┬ input_layernorm
      │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].input_layernorm
      │  │     |  ├─ children: ['weight']
      │  │     │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │     ├──┬ mlp
      │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].mlp
      │  │     |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
      │  │     │  ├──┬ down_proj
      │  │     │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].mlp.down_proj
      │  │     │  |  ├─ children: ['weight']
      │  │     │  │  └── weight: torch.Size([720, 2048]) [torch.bfloat16]
      │  │     │  ├──┬ gate_proj
      │  │     │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].mlp.gate_proj
      │  │     │  |  ├─ children: ['weight']
      │  │     │  │  └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │     │  └──┬ up_proj:
      │  │     │     ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].mlp.up_proj
      │  │     │     ├─ children: ['weight']
      │  │     │     └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │     ├──┬ post_attention_layernorm
      │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].post_attention_layernorm
      │  │     |  ├─ children: ['weight']
      │  │     │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │     └──┬ self_attn:
      │  │        ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].self_attn
      │  │        ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
      │  │        ├──┬ k_proj
      │  │        |  ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].self_attn.k_proj
      │  │        |  ├─ children: ['weight']
      │  │        │  └── weight: torch.Size([320, 720]) [torch.bfloat16]
      │  │        ├──┬ o_proj
      │  │        |  ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].self_attn.o_proj
      │  │        |  ├─ children: ['weight']
      │  │        │  └── weight: torch.Size([720, 960]) [torch.bfloat16]
      │  │        ├──┬ q_proj
      │  │        |  ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].self_attn.q_proj
      │  │        |  ├─ children: ['weight']
      │  │        │  └── weight: torch.Size([960, 720]) [torch.bfloat16]
      │  │        └──┬ v_proj:
      │  │           ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].self_attn.v_proj
      │  │           ├─ children: ['weight']
      │  │           └── weight: torch.Size([320, 720]) [torch.bfloat16]
      │  └──┬ norm:
      │     ├─ in: .model.vlm_with_expert.lm_expert.norm
      │     ├─ children: ['weight']
      │     └── weight: torch.Size([720]) [torch.bfloat16]
      └──┬ vlm:
         ├─ in: .model.vlm_with_expert.vlm
         ├─ children: ['lm_head', 'model']
         ├──┬ lm_head
         |  ├─ in: .model.vlm_with_expert.vlm.lm_head
         |  ├─ children: ['weight']
         │  └── weight: torch.Size([49280, 960]) [torch.bfloat16]
         └──┬ model:
            ├─ in: .model.vlm_with_expert.vlm.model
            ├─ children: ['connector', 'text_model', 'vision_model']
            ├──┬ connector
            |  ├─ in: .model.vlm_with_expert.vlm.model.connector
            |  ├─ children: ['modality_projection']
            │  └──┬ modality_projection:
            │     ├─ in: .model.vlm_with_expert.vlm.model.connector.modality_projection
            │     ├─ children: ['proj']
            │     └──┬ proj:
            │        ├─ in: .model.vlm_with_expert.vlm.model.connector.modality_projection.proj
            │        ├─ children: ['weight']
            │        └── weight: torch.Size([960, 12288]) [torch.bfloat16]
            ├──┬ text_model
            |  ├─ in: .model.vlm_with_expert.vlm.model.text_model
            |  ├─ children: ['embed_tokens', 'layers', 'norm']
            │  ├──┬ embed_tokens
            │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.embed_tokens
            │  |  ├─ children: ['weight']
            │  │  └── weight: torch.Size([49280, 960]) [torch.bfloat16]
            │  ├──┬ layers
            │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers
            │  |  ├─ children: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15']
            │  │  └──┬ [0..15]:
            │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15]
            │  │     ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
            │  │     ├──┬ input_layernorm
            │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].input_layernorm
            │  │     |  ├─ children: ['weight']
            │  │     │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │     ├──┬ mlp
            │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].mlp
            │  │     |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
            │  │     │  ├──┬ down_proj
            │  │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].mlp.down_proj
            │  │     │  |  ├─ children: ['weight']
            │  │     │  │  └── weight: torch.Size([960, 2560]) [torch.bfloat16]
            │  │     │  ├──┬ gate_proj
            │  │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].mlp.gate_proj
            │  │     │  |  ├─ children: ['weight']
            │  │     │  │  └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │     │  └──┬ up_proj:
            │  │     │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].mlp.up_proj
            │  │     │     ├─ children: ['weight']
            │  │     │     └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │     ├──┬ post_attention_layernorm
            │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].post_attention_layernorm
            │  │     |  ├─ children: ['weight']
            │  │     │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │     └──┬ self_attn:
            │  │        ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].self_attn
            │  │        ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
            │  │        ├──┬ k_proj
            │  │        |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].self_attn.k_proj
            │  │        |  ├─ children: ['weight']
            │  │        │  └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │        ├──┬ o_proj
            │  │        |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].self_attn.o_proj
            │  │        |  ├─ children: ['weight']
            │  │        │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │        ├──┬ q_proj
            │  │        |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].self_attn.q_proj
            │  │        |  ├─ children: ['weight']
            │  │        │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │        └──┬ v_proj:
            │  │           ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].self_attn.v_proj
            │  │           ├─ children: ['weight']
            │  │           └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  └──┬ norm:
            │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.norm
            │     ├─ children: ['weight']
            │     └── weight: torch.Size([960]) [torch.bfloat16]
            └──┬ vision_model:
               ├─ in: .model.vlm_with_expert.vlm.model.vision_model
               ├─ children: ['embeddings', 'encoder', 'post_layernorm']
               ├──┬ embeddings
               |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.embeddings
               |  ├─ children: ['patch_embedding', 'position_embedding']
               │  ├──┬ patch_embedding
               │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.embeddings.patch_embedding
               │  |  ├─ children: ['bias', 'weight']
               │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │  │  └── weight: torch.Size([768, 3, 16, 16]) [torch.bfloat16]
               │  └──┬ position_embedding:
               │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.embeddings.position_embedding
               │     ├─ children: ['weight']
               │     └── weight: torch.Size([1024, 768]) [torch.bfloat16]
               ├──┬ encoder
               |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder
               |  ├─ children: ['layers']
               │  └──┬ layers:
               │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers
               │     ├─ children: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
               │     └──┬ [0..11]:
               │        ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11]
               │        ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']
               │        ├──┬ layer_norm1
               │        |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].layer_norm1
               │        |  ├─ children: ['bias', 'weight']
               │        │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │        │  └── weight: torch.Size([768]) [torch.bfloat16]
               │        ├──┬ layer_norm2
               │        |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].layer_norm2
               │        |  ├─ children: ['bias', 'weight']
               │        │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │        │  └── weight: torch.Size([768]) [torch.bfloat16]
               │        ├──┬ mlp
               │        |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].mlp
               │        |  ├─ children: ['fc1', 'fc2']
               │        │  ├──┬ fc1
               │        │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].mlp.fc1
               │        │  |  ├─ children: ['bias', 'weight']
               │        │  │  ├── bias: torch.Size([3072]) [torch.bfloat16]
               │        │  │  └── weight: torch.Size([3072, 768]) [torch.bfloat16]
               │        │  └──┬ fc2:
               │        │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].mlp.fc2
               │        │     ├─ children: ['bias', 'weight']
               │        │     ├── bias: torch.Size([768]) [torch.bfloat16]
               │        │     └── weight: torch.Size([768, 3072]) [torch.bfloat16]
               │        └──┬ self_attn:
               │           ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].self_attn
               │           ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']
               │           ├──┬ k_proj
               │           |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].self_attn.k_proj
               │           |  ├─ children: ['bias', 'weight']
               │           │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │           │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │           ├──┬ out_proj
               │           |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].self_attn.out_proj
               │           |  ├─ children: ['bias', 'weight']
               │           │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │           │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │           ├──┬ q_proj
               │           |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].self_attn.q_proj
               │           |  ├─ children: ['bias', 'weight']
               │           │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │           │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │           └──┬ v_proj:
               │              ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].self_attn.v_proj
               │              ├─ children: ['bias', 'weight']
               │              ├── bias: torch.Size([768]) [torch.bfloat16]
               │              └── weight: torch.Size([768, 768]) [torch.bfloat16]
               └──┬ post_layernorm:
                  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.post_layernorm
                  ├─ children: ['bias', 'weight']
                  ├── bias: torch.Size([768]) [torch.bfloat16]
                  └── weight: torch.Size([768]) [torch.bfloat16]
