└──┬ model:
   ├─ children: ['action_in_proj', 'action_out_proj', 'action_time_mlp_in', 'action_time_mlp_out', 'state_proj', 'vlm_with_expert']
   ├──┬ action_in_proj
   |  ├─ in: .model.action_in_proj
   |  ├─ children: ['bias', 'weight']
   │  ├── bias: torch.Size([720]) [torch.float32]
   │  └── weight: torch.Size([720, 32]) [torch.float32]
   ├──┬ action_out_proj
   |  ├─ in: .model.action_out_proj
   |  ├─ children: ['bias', 'weight']
   │  ├── bias: torch.Size([32]) [torch.float32]
   │  └── weight: torch.Size([32, 720]) [torch.float32]
   ├──┬ action_time_mlp_in
   |  ├─ in: .model.action_time_mlp_in
   |  ├─ children: ['bias', 'weight']
   │  ├── bias: torch.Size([720]) [torch.float32]
   │  └── weight: torch.Size([720, 1440]) [torch.float32]
   ├──┬ action_time_mlp_out
   |  ├─ in: .model.action_time_mlp_out
   |  ├─ children: ['bias', 'weight']
   │  ├── bias: torch.Size([720]) [torch.float32]
   │  └── weight: torch.Size([720, 720]) [torch.float32]
   ├──┬ state_proj
   |  ├─ in: .model.state_proj
   |  ├─ children: ['bias', 'weight']
   │  ├── bias: torch.Size([960]) [torch.float32]
   │  └── weight: torch.Size([960, 32]) [torch.float32]
   └──┬ vlm_with_expert:
      ├─ in: .model.vlm_with_expert
      ├─ children: ['lm_expert', 'vlm']
      ├──┬ lm_expert
      |  ├─ in: .model.vlm_with_expert.lm_expert
      |  ├─ children: ['layers', 'norm']
      │  ├──┬ layers
      │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers
      │  |  ├─ children: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15']
      │  │  ├──┬ 0
      │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.0
      │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
      │  │  │  ├──┬ input_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.0.input_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  ├──┬ mlp
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.0.mlp
      │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
      │  │  │  │  ├──┬ down_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.0.mlp.down_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 2048]) [torch.bfloat16]
      │  │  │  │  ├──┬ gate_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.0.mlp.gate_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  │  └──┬ up_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.0.mlp.up_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  ├──┬ post_attention_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.0.post_attention_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  └──┬ self_attn:
      │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.0.self_attn
      │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
      │  │  │     ├──┬ k_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.0.self_attn.k_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([320, 720]) [torch.bfloat16]
      │  │  │     ├──┬ o_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.0.self_attn.o_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([720, 960]) [torch.bfloat16]
      │  │  │     ├──┬ q_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.0.self_attn.q_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([960, 720]) [torch.bfloat16]
      │  │  │     └──┬ v_proj:
      │  │  │        ├─ in: .model.vlm_with_expert.lm_expert.layers.0.self_attn.v_proj
      │  │  │        ├─ children: ['weight']
      │  │  │        └── weight: torch.Size([320, 720]) [torch.bfloat16]
      │  │  ├──┬ 1
      │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.1
      │  │  |  ├─ children: ['self_attn', 'input_layernorm', 'mlp', 'post_attention_layernorm']
      │  │  │  ├──┬ self_attn
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.1.self_attn
      │  │  │  |  ├─ children: ['k_proj', 'v_proj', 'o_proj', 'q_proj']
      │  │  │  │  ├──┬ k_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.1.self_attn.k_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([320, 320]) [torch.float32]
      │  │  │  │  ├──┬ v_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.1.self_attn.v_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([320, 320]) [torch.float32]
      │  │  │  │  ├──┬ o_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.1.self_attn.o_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 960]) [torch.bfloat16]
      │  │  │  │  └──┬ q_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.1.self_attn.q_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([960, 720]) [torch.bfloat16]
      │  │  │  ├──┬ input_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.1.input_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  ├──┬ mlp
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.1.mlp
      │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
      │  │  │  │  ├──┬ down_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.1.mlp.down_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 2048]) [torch.bfloat16]
      │  │  │  │  ├──┬ gate_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.1.mlp.gate_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  │  └──┬ up_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.1.mlp.up_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  └──┬ post_attention_layernorm:
      │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.1.post_attention_layernorm
      │  │  │     ├─ children: ['weight']
      │  │  │     └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  ├──┬ 2
      │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.2
      │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
      │  │  │  ├──┬ input_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.2.input_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  ├──┬ mlp
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.2.mlp
      │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
      │  │  │  │  ├──┬ down_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.2.mlp.down_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 2048]) [torch.bfloat16]
      │  │  │  │  ├──┬ gate_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.2.mlp.gate_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  │  └──┬ up_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.2.mlp.up_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  ├──┬ post_attention_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.2.post_attention_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  └──┬ self_attn:
      │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.2.self_attn
      │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
      │  │  │     ├──┬ k_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.2.self_attn.k_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([320, 720]) [torch.bfloat16]
      │  │  │     ├──┬ o_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.2.self_attn.o_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([720, 960]) [torch.bfloat16]
      │  │  │     ├──┬ q_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.2.self_attn.q_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([960, 720]) [torch.bfloat16]
      │  │  │     └──┬ v_proj:
      │  │  │        ├─ in: .model.vlm_with_expert.lm_expert.layers.2.self_attn.v_proj
      │  │  │        ├─ children: ['weight']
      │  │  │        └── weight: torch.Size([320, 720]) [torch.bfloat16]
      │  │  ├──┬ 3
      │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.3
      │  │  |  ├─ children: ['self_attn', 'input_layernorm', 'mlp', 'post_attention_layernorm']
      │  │  │  ├──┬ self_attn
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.3.self_attn
      │  │  │  |  ├─ children: ['k_proj', 'v_proj', 'o_proj', 'q_proj']
      │  │  │  │  ├──┬ k_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.3.self_attn.k_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([320, 320]) [torch.float32]
      │  │  │  │  ├──┬ v_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.3.self_attn.v_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([320, 320]) [torch.float32]
      │  │  │  │  ├──┬ o_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.3.self_attn.o_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 960]) [torch.bfloat16]
      │  │  │  │  └──┬ q_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.3.self_attn.q_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([960, 720]) [torch.bfloat16]
      │  │  │  ├──┬ input_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.3.input_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  ├──┬ mlp
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.3.mlp
      │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
      │  │  │  │  ├──┬ down_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.3.mlp.down_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 2048]) [torch.bfloat16]
      │  │  │  │  ├──┬ gate_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.3.mlp.gate_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  │  └──┬ up_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.3.mlp.up_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  └──┬ post_attention_layernorm:
      │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.3.post_attention_layernorm
      │  │  │     ├─ children: ['weight']
      │  │  │     └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  ├──┬ 4
      │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.4
      │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
      │  │  │  ├──┬ input_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.4.input_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  ├──┬ mlp
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.4.mlp
      │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
      │  │  │  │  ├──┬ down_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.4.mlp.down_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 2048]) [torch.bfloat16]
      │  │  │  │  ├──┬ gate_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.4.mlp.gate_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  │  └──┬ up_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.4.mlp.up_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  ├──┬ post_attention_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.4.post_attention_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  └──┬ self_attn:
      │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.4.self_attn
      │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
      │  │  │     ├──┬ k_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.4.self_attn.k_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([320, 720]) [torch.bfloat16]
      │  │  │     ├──┬ o_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.4.self_attn.o_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([720, 960]) [torch.bfloat16]
      │  │  │     ├──┬ q_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.4.self_attn.q_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([960, 720]) [torch.bfloat16]
      │  │  │     └──┬ v_proj:
      │  │  │        ├─ in: .model.vlm_with_expert.lm_expert.layers.4.self_attn.v_proj
      │  │  │        ├─ children: ['weight']
      │  │  │        └── weight: torch.Size([320, 720]) [torch.bfloat16]
      │  │  ├──┬ 5
      │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.5
      │  │  |  ├─ children: ['self_attn', 'input_layernorm', 'mlp', 'post_attention_layernorm']
      │  │  │  ├──┬ self_attn
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.5.self_attn
      │  │  │  |  ├─ children: ['k_proj', 'v_proj', 'o_proj', 'q_proj']
      │  │  │  │  ├──┬ k_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.5.self_attn.k_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([320, 320]) [torch.float32]
      │  │  │  │  ├──┬ v_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.5.self_attn.v_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([320, 320]) [torch.float32]
      │  │  │  │  ├──┬ o_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.5.self_attn.o_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 960]) [torch.bfloat16]
      │  │  │  │  └──┬ q_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.5.self_attn.q_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([960, 720]) [torch.bfloat16]
      │  │  │  ├──┬ input_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.5.input_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  ├──┬ mlp
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.5.mlp
      │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
      │  │  │  │  ├──┬ down_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.5.mlp.down_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 2048]) [torch.bfloat16]
      │  │  │  │  ├──┬ gate_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.5.mlp.gate_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  │  └──┬ up_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.5.mlp.up_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  └──┬ post_attention_layernorm:
      │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.5.post_attention_layernorm
      │  │  │     ├─ children: ['weight']
      │  │  │     └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  ├──┬ 6
      │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.6
      │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
      │  │  │  ├──┬ input_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.6.input_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  ├──┬ mlp
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.6.mlp
      │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
      │  │  │  │  ├──┬ down_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.6.mlp.down_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 2048]) [torch.bfloat16]
      │  │  │  │  ├──┬ gate_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.6.mlp.gate_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  │  └──┬ up_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.6.mlp.up_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  ├──┬ post_attention_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.6.post_attention_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  └──┬ self_attn:
      │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.6.self_attn
      │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
      │  │  │     ├──┬ k_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.6.self_attn.k_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([320, 720]) [torch.bfloat16]
      │  │  │     ├──┬ o_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.6.self_attn.o_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([720, 960]) [torch.bfloat16]
      │  │  │     ├──┬ q_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.6.self_attn.q_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([960, 720]) [torch.bfloat16]
      │  │  │     └──┬ v_proj:
      │  │  │        ├─ in: .model.vlm_with_expert.lm_expert.layers.6.self_attn.v_proj
      │  │  │        ├─ children: ['weight']
      │  │  │        └── weight: torch.Size([320, 720]) [torch.bfloat16]
      │  │  ├──┬ 7
      │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.7
      │  │  |  ├─ children: ['self_attn', 'input_layernorm', 'mlp', 'post_attention_layernorm']
      │  │  │  ├──┬ self_attn
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.7.self_attn
      │  │  │  |  ├─ children: ['k_proj', 'v_proj', 'o_proj', 'q_proj']
      │  │  │  │  ├──┬ k_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.7.self_attn.k_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([320, 320]) [torch.float32]
      │  │  │  │  ├──┬ v_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.7.self_attn.v_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([320, 320]) [torch.float32]
      │  │  │  │  ├──┬ o_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.7.self_attn.o_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 960]) [torch.bfloat16]
      │  │  │  │  └──┬ q_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.7.self_attn.q_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([960, 720]) [torch.bfloat16]
      │  │  │  ├──┬ input_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.7.input_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  ├──┬ mlp
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.7.mlp
      │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
      │  │  │  │  ├──┬ down_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.7.mlp.down_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 2048]) [torch.bfloat16]
      │  │  │  │  ├──┬ gate_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.7.mlp.gate_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  │  └──┬ up_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.7.mlp.up_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  └──┬ post_attention_layernorm:
      │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.7.post_attention_layernorm
      │  │  │     ├─ children: ['weight']
      │  │  │     └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  ├──┬ 8
      │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.8
      │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
      │  │  │  ├──┬ input_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.8.input_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  ├──┬ mlp
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.8.mlp
      │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
      │  │  │  │  ├──┬ down_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.8.mlp.down_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 2048]) [torch.bfloat16]
      │  │  │  │  ├──┬ gate_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.8.mlp.gate_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  │  └──┬ up_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.8.mlp.up_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  ├──┬ post_attention_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.8.post_attention_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  └──┬ self_attn:
      │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.8.self_attn
      │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
      │  │  │     ├──┬ k_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.8.self_attn.k_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([320, 720]) [torch.bfloat16]
      │  │  │     ├──┬ o_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.8.self_attn.o_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([720, 960]) [torch.bfloat16]
      │  │  │     ├──┬ q_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.8.self_attn.q_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([960, 720]) [torch.bfloat16]
      │  │  │     └──┬ v_proj:
      │  │  │        ├─ in: .model.vlm_with_expert.lm_expert.layers.8.self_attn.v_proj
      │  │  │        ├─ children: ['weight']
      │  │  │        └── weight: torch.Size([320, 720]) [torch.bfloat16]
      │  │  ├──┬ 9
      │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.9
      │  │  |  ├─ children: ['self_attn', 'input_layernorm', 'mlp', 'post_attention_layernorm']
      │  │  │  ├──┬ self_attn
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.9.self_attn
      │  │  │  |  ├─ children: ['k_proj', 'v_proj', 'o_proj', 'q_proj']
      │  │  │  │  ├──┬ k_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.9.self_attn.k_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([320, 320]) [torch.float32]
      │  │  │  │  ├──┬ v_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.9.self_attn.v_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([320, 320]) [torch.float32]
      │  │  │  │  ├──┬ o_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.9.self_attn.o_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 960]) [torch.bfloat16]
      │  │  │  │  └──┬ q_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.9.self_attn.q_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([960, 720]) [torch.bfloat16]
      │  │  │  ├──┬ input_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.9.input_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  ├──┬ mlp
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.9.mlp
      │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
      │  │  │  │  ├──┬ down_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.9.mlp.down_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 2048]) [torch.bfloat16]
      │  │  │  │  ├──┬ gate_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.9.mlp.gate_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  │  └──┬ up_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.9.mlp.up_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  └──┬ post_attention_layernorm:
      │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.9.post_attention_layernorm
      │  │  │     ├─ children: ['weight']
      │  │  │     └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  ├──┬ 10
      │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.10
      │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
      │  │  │  ├──┬ input_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.10.input_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  ├──┬ mlp
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.10.mlp
      │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
      │  │  │  │  ├──┬ down_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.10.mlp.down_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 2048]) [torch.bfloat16]
      │  │  │  │  ├──┬ gate_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.10.mlp.gate_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  │  └──┬ up_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.10.mlp.up_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  ├──┬ post_attention_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.10.post_attention_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  └──┬ self_attn:
      │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.10.self_attn
      │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
      │  │  │     ├──┬ k_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.10.self_attn.k_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([320, 720]) [torch.bfloat16]
      │  │  │     ├──┬ o_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.10.self_attn.o_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([720, 960]) [torch.bfloat16]
      │  │  │     ├──┬ q_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.10.self_attn.q_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([960, 720]) [torch.bfloat16]
      │  │  │     └──┬ v_proj:
      │  │  │        ├─ in: .model.vlm_with_expert.lm_expert.layers.10.self_attn.v_proj
      │  │  │        ├─ children: ['weight']
      │  │  │        └── weight: torch.Size([320, 720]) [torch.bfloat16]
      │  │  ├──┬ 11
      │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.11
      │  │  |  ├─ children: ['self_attn', 'input_layernorm', 'mlp', 'post_attention_layernorm']
      │  │  │  ├──┬ self_attn
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.11.self_attn
      │  │  │  |  ├─ children: ['k_proj', 'v_proj', 'o_proj', 'q_proj']
      │  │  │  │  ├──┬ k_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.11.self_attn.k_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([320, 320]) [torch.float32]
      │  │  │  │  ├──┬ v_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.11.self_attn.v_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([320, 320]) [torch.float32]
      │  │  │  │  ├──┬ o_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.11.self_attn.o_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 960]) [torch.bfloat16]
      │  │  │  │  └──┬ q_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.11.self_attn.q_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([960, 720]) [torch.bfloat16]
      │  │  │  ├──┬ input_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.11.input_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  ├──┬ mlp
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.11.mlp
      │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
      │  │  │  │  ├──┬ down_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.11.mlp.down_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 2048]) [torch.bfloat16]
      │  │  │  │  ├──┬ gate_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.11.mlp.gate_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  │  └──┬ up_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.11.mlp.up_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  └──┬ post_attention_layernorm:
      │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.11.post_attention_layernorm
      │  │  │     ├─ children: ['weight']
      │  │  │     └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  ├──┬ 12
      │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.12
      │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
      │  │  │  ├──┬ input_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.12.input_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  ├──┬ mlp
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.12.mlp
      │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
      │  │  │  │  ├──┬ down_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.12.mlp.down_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 2048]) [torch.bfloat16]
      │  │  │  │  ├──┬ gate_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.12.mlp.gate_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  │  └──┬ up_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.12.mlp.up_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  ├──┬ post_attention_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.12.post_attention_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  └──┬ self_attn:
      │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.12.self_attn
      │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
      │  │  │     ├──┬ k_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.12.self_attn.k_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([320, 720]) [torch.bfloat16]
      │  │  │     ├──┬ o_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.12.self_attn.o_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([720, 960]) [torch.bfloat16]
      │  │  │     ├──┬ q_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.12.self_attn.q_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([960, 720]) [torch.bfloat16]
      │  │  │     └──┬ v_proj:
      │  │  │        ├─ in: .model.vlm_with_expert.lm_expert.layers.12.self_attn.v_proj
      │  │  │        ├─ children: ['weight']
      │  │  │        └── weight: torch.Size([320, 720]) [torch.bfloat16]
      │  │  ├──┬ 13
      │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.13
      │  │  |  ├─ children: ['self_attn', 'input_layernorm', 'mlp', 'post_attention_layernorm']
      │  │  │  ├──┬ self_attn
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.13.self_attn
      │  │  │  |  ├─ children: ['k_proj', 'v_proj', 'o_proj', 'q_proj']
      │  │  │  │  ├──┬ k_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.13.self_attn.k_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([320, 320]) [torch.float32]
      │  │  │  │  ├──┬ v_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.13.self_attn.v_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([320, 320]) [torch.float32]
      │  │  │  │  ├──┬ o_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.13.self_attn.o_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 960]) [torch.bfloat16]
      │  │  │  │  └──┬ q_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.13.self_attn.q_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([960, 720]) [torch.bfloat16]
      │  │  │  ├──┬ input_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.13.input_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  ├──┬ mlp
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.13.mlp
      │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
      │  │  │  │  ├──┬ down_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.13.mlp.down_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 2048]) [torch.bfloat16]
      │  │  │  │  ├──┬ gate_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.13.mlp.gate_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  │  └──┬ up_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.13.mlp.up_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  └──┬ post_attention_layernorm:
      │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.13.post_attention_layernorm
      │  │  │     ├─ children: ['weight']
      │  │  │     └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  ├──┬ 14
      │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.14
      │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
      │  │  │  ├──┬ input_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.14.input_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  ├──┬ mlp
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.14.mlp
      │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
      │  │  │  │  ├──┬ down_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.14.mlp.down_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([720, 2048]) [torch.bfloat16]
      │  │  │  │  ├──┬ gate_proj
      │  │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.14.mlp.gate_proj
      │  │  │  │  |  ├─ children: ['weight']
      │  │  │  │  │  └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  │  └──┬ up_proj:
      │  │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.14.mlp.up_proj
      │  │  │  │     ├─ children: ['weight']
      │  │  │  │     └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │  │  ├──┬ post_attention_layernorm
      │  │  │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.14.post_attention_layernorm
      │  │  │  |  ├─ children: ['weight']
      │  │  │  │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │  │  └──┬ self_attn:
      │  │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.14.self_attn
      │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
      │  │  │     ├──┬ k_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.14.self_attn.k_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([320, 720]) [torch.bfloat16]
      │  │  │     ├──┬ o_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.14.self_attn.o_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([720, 960]) [torch.bfloat16]
      │  │  │     ├──┬ q_proj
      │  │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.14.self_attn.q_proj
      │  │  │     |  ├─ children: ['weight']
      │  │  │     │  └── weight: torch.Size([960, 720]) [torch.bfloat16]
      │  │  │     └──┬ v_proj:
      │  │  │        ├─ in: .model.vlm_with_expert.lm_expert.layers.14.self_attn.v_proj
      │  │  │        ├─ children: ['weight']
      │  │  │        └── weight: torch.Size([320, 720]) [torch.bfloat16]
      │  │  └──┬ 15:
      │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.15
      │  │     ├─ children: ['self_attn', 'input_layernorm', 'mlp', 'post_attention_layernorm']
      │  │     ├──┬ self_attn
      │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.15.self_attn
      │  │     |  ├─ children: ['k_proj', 'v_proj', 'o_proj', 'q_proj']
      │  │     │  ├──┬ k_proj
      │  │     │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.15.self_attn.k_proj
      │  │     │  |  ├─ children: ['weight']
      │  │     │  │  └── weight: torch.Size([320, 320]) [torch.float32]
      │  │     │  ├──┬ v_proj
      │  │     │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.15.self_attn.v_proj
      │  │     │  |  ├─ children: ['weight']
      │  │     │  │  └── weight: torch.Size([320, 320]) [torch.float32]
      │  │     │  ├──┬ o_proj
      │  │     │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.15.self_attn.o_proj
      │  │     │  |  ├─ children: ['weight']
      │  │     │  │  └── weight: torch.Size([720, 960]) [torch.bfloat16]
      │  │     │  └──┬ q_proj:
      │  │     │     ├─ in: .model.vlm_with_expert.lm_expert.layers.15.self_attn.q_proj
      │  │     │     ├─ children: ['weight']
      │  │     │     └── weight: torch.Size([960, 720]) [torch.bfloat16]
      │  │     ├──┬ input_layernorm
      │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.15.input_layernorm
      │  │     |  ├─ children: ['weight']
      │  │     │  └── weight: torch.Size([720]) [torch.bfloat16]
      │  │     ├──┬ mlp
      │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.15.mlp
      │  │     |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
      │  │     │  ├──┬ down_proj
      │  │     │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.15.mlp.down_proj
      │  │     │  |  ├─ children: ['weight']
      │  │     │  │  └── weight: torch.Size([720, 2048]) [torch.bfloat16]
      │  │     │  ├──┬ gate_proj
      │  │     │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.15.mlp.gate_proj
      │  │     │  |  ├─ children: ['weight']
      │  │     │  │  └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │     │  └──┬ up_proj:
      │  │     │     ├─ in: .model.vlm_with_expert.lm_expert.layers.15.mlp.up_proj
      │  │     │     ├─ children: ['weight']
      │  │     │     └── weight: torch.Size([2048, 720]) [torch.bfloat16]
      │  │     └──┬ post_attention_layernorm:
      │  │        ├─ in: .model.vlm_with_expert.lm_expert.layers.15.post_attention_layernorm
      │  │        ├─ children: ['weight']
      │  │        └── weight: torch.Size([720]) [torch.bfloat16]
      │  └──┬ norm:
      │     ├─ in: .model.vlm_with_expert.lm_expert.norm
      │     ├─ children: ['weight']
      │     └── weight: torch.Size([720]) [torch.bfloat16]
      └──┬ vlm:
         ├─ in: .model.vlm_with_expert.vlm
         ├─ children: ['lm_head', 'model']
         ├──┬ lm_head
         |  ├─ in: .model.vlm_with_expert.vlm.lm_head
         |  ├─ children: ['weight']
         │  └── weight: torch.Size([49280, 960]) [torch.bfloat16]
         └──┬ model:
            ├─ in: .model.vlm_with_expert.vlm.model
            ├─ children: ['connector', 'text_model', 'vision_model']
            ├──┬ connector
            |  ├─ in: .model.vlm_with_expert.vlm.model.connector
            |  ├─ children: ['modality_projection']
            │  └──┬ modality_projection:
            │     ├─ in: .model.vlm_with_expert.vlm.model.connector.modality_projection
            │     ├─ children: ['proj']
            │     └──┬ proj:
            │        ├─ in: .model.vlm_with_expert.vlm.model.connector.modality_projection.proj
            │        ├─ children: ['weight']
            │        └── weight: torch.Size([960, 12288]) [torch.bfloat16]
            ├──┬ text_model
            |  ├─ in: .model.vlm_with_expert.vlm.model.text_model
            |  ├─ children: ['embed_tokens', 'layers', 'norm']
            │  ├──┬ embed_tokens
            │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.embed_tokens
            │  |  ├─ children: ['weight']
            │  │  └── weight: torch.Size([49280, 960]) [torch.bfloat16]
            │  ├──┬ layers
            │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers
            │  |  ├─ children: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15']
            │  │  ├──┬ 0
            │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0
            │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
            │  │  │  ├──┬ input_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.input_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  ├──┬ mlp
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.mlp
            │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
            │  │  │  │  ├──┬ down_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.down_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([960, 2560]) [torch.bfloat16]
            │  │  │  │  ├──┬ gate_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.gate_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  │  └──┬ up_proj:
            │  │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.up_proj
            │  │  │  │     ├─ children: ['weight']
            │  │  │  │     └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  ├──┬ post_attention_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.post_attention_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  └──┬ self_attn:
            │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn
            │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
            │  │  │     ├──┬ k_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.k_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  │     ├──┬ o_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.o_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     ├──┬ q_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.q_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     └──┬ v_proj:
            │  │  │        ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.v_proj
            │  │  │        ├─ children: ['weight']
            │  │  │        └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  ├──┬ 1
            │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1
            │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
            │  │  │  ├──┬ input_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.input_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  ├──┬ mlp
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.mlp
            │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
            │  │  │  │  ├──┬ down_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.down_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([960, 2560]) [torch.bfloat16]
            │  │  │  │  ├──┬ gate_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.gate_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  │  └──┬ up_proj:
            │  │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.up_proj
            │  │  │  │     ├─ children: ['weight']
            │  │  │  │     └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  ├──┬ post_attention_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.post_attention_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  └──┬ self_attn:
            │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn
            │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
            │  │  │     ├──┬ k_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.k_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  │     ├──┬ o_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.o_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     ├──┬ q_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.q_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     └──┬ v_proj:
            │  │  │        ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.v_proj
            │  │  │        ├─ children: ['weight']
            │  │  │        └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  ├──┬ 2
            │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2
            │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
            │  │  │  ├──┬ input_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.input_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  ├──┬ mlp
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.mlp
            │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
            │  │  │  │  ├──┬ down_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.down_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([960, 2560]) [torch.bfloat16]
            │  │  │  │  ├──┬ gate_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.gate_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  │  └──┬ up_proj:
            │  │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.up_proj
            │  │  │  │     ├─ children: ['weight']
            │  │  │  │     └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  ├──┬ post_attention_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.post_attention_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  └──┬ self_attn:
            │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn
            │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
            │  │  │     ├──┬ k_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.k_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  │     ├──┬ o_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.o_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     ├──┬ q_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.q_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     └──┬ v_proj:
            │  │  │        ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.v_proj
            │  │  │        ├─ children: ['weight']
            │  │  │        └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  ├──┬ 3
            │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3
            │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
            │  │  │  ├──┬ input_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.input_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  ├──┬ mlp
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.mlp
            │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
            │  │  │  │  ├──┬ down_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.down_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([960, 2560]) [torch.bfloat16]
            │  │  │  │  ├──┬ gate_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.gate_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  │  └──┬ up_proj:
            │  │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.up_proj
            │  │  │  │     ├─ children: ['weight']
            │  │  │  │     └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  ├──┬ post_attention_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.post_attention_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  └──┬ self_attn:
            │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn
            │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
            │  │  │     ├──┬ k_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.k_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  │     ├──┬ o_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.o_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     ├──┬ q_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.q_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     └──┬ v_proj:
            │  │  │        ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.v_proj
            │  │  │        ├─ children: ['weight']
            │  │  │        └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  ├──┬ 4
            │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4
            │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
            │  │  │  ├──┬ input_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.input_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  ├──┬ mlp
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.mlp
            │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
            │  │  │  │  ├──┬ down_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.down_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([960, 2560]) [torch.bfloat16]
            │  │  │  │  ├──┬ gate_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.gate_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  │  └──┬ up_proj:
            │  │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.up_proj
            │  │  │  │     ├─ children: ['weight']
            │  │  │  │     └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  ├──┬ post_attention_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.post_attention_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  └──┬ self_attn:
            │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn
            │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
            │  │  │     ├──┬ k_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.k_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  │     ├──┬ o_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.o_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     ├──┬ q_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.q_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     └──┬ v_proj:
            │  │  │        ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.v_proj
            │  │  │        ├─ children: ['weight']
            │  │  │        └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  ├──┬ 5
            │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5
            │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
            │  │  │  ├──┬ input_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.input_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  ├──┬ mlp
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.mlp
            │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
            │  │  │  │  ├──┬ down_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.down_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([960, 2560]) [torch.bfloat16]
            │  │  │  │  ├──┬ gate_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.gate_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  │  └──┬ up_proj:
            │  │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.up_proj
            │  │  │  │     ├─ children: ['weight']
            │  │  │  │     └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  ├──┬ post_attention_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.post_attention_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  └──┬ self_attn:
            │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn
            │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
            │  │  │     ├──┬ k_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.k_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  │     ├──┬ o_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.o_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     ├──┬ q_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.q_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     └──┬ v_proj:
            │  │  │        ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.v_proj
            │  │  │        ├─ children: ['weight']
            │  │  │        └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  ├──┬ 6
            │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6
            │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
            │  │  │  ├──┬ input_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.input_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  ├──┬ mlp
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.mlp
            │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
            │  │  │  │  ├──┬ down_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.down_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([960, 2560]) [torch.bfloat16]
            │  │  │  │  ├──┬ gate_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.gate_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  │  └──┬ up_proj:
            │  │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.up_proj
            │  │  │  │     ├─ children: ['weight']
            │  │  │  │     └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  ├──┬ post_attention_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.post_attention_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  └──┬ self_attn:
            │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn
            │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
            │  │  │     ├──┬ k_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.k_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  │     ├──┬ o_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.o_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     ├──┬ q_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.q_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     └──┬ v_proj:
            │  │  │        ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.v_proj
            │  │  │        ├─ children: ['weight']
            │  │  │        └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  ├──┬ 7
            │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7
            │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
            │  │  │  ├──┬ input_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.input_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  ├──┬ mlp
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.mlp
            │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
            │  │  │  │  ├──┬ down_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.down_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([960, 2560]) [torch.bfloat16]
            │  │  │  │  ├──┬ gate_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.gate_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  │  └──┬ up_proj:
            │  │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.up_proj
            │  │  │  │     ├─ children: ['weight']
            │  │  │  │     └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  ├──┬ post_attention_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.post_attention_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  └──┬ self_attn:
            │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn
            │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
            │  │  │     ├──┬ k_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.k_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  │     ├──┬ o_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.o_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     ├──┬ q_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.q_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     └──┬ v_proj:
            │  │  │        ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.v_proj
            │  │  │        ├─ children: ['weight']
            │  │  │        └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  ├──┬ 8
            │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8
            │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
            │  │  │  ├──┬ input_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.input_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  ├──┬ mlp
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.mlp
            │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
            │  │  │  │  ├──┬ down_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.down_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([960, 2560]) [torch.bfloat16]
            │  │  │  │  ├──┬ gate_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.gate_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  │  └──┬ up_proj:
            │  │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.up_proj
            │  │  │  │     ├─ children: ['weight']
            │  │  │  │     └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  ├──┬ post_attention_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.post_attention_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  └──┬ self_attn:
            │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn
            │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
            │  │  │     ├──┬ k_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.k_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  │     ├──┬ o_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.o_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     ├──┬ q_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.q_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     └──┬ v_proj:
            │  │  │        ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.v_proj
            │  │  │        ├─ children: ['weight']
            │  │  │        └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  ├──┬ 9
            │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9
            │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
            │  │  │  ├──┬ input_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.input_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  ├──┬ mlp
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.mlp
            │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
            │  │  │  │  ├──┬ down_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.down_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([960, 2560]) [torch.bfloat16]
            │  │  │  │  ├──┬ gate_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.gate_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  │  └──┬ up_proj:
            │  │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.up_proj
            │  │  │  │     ├─ children: ['weight']
            │  │  │  │     └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  ├──┬ post_attention_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.post_attention_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  └──┬ self_attn:
            │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn
            │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
            │  │  │     ├──┬ k_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.k_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  │     ├──┬ o_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.o_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     ├──┬ q_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.q_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     └──┬ v_proj:
            │  │  │        ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.v_proj
            │  │  │        ├─ children: ['weight']
            │  │  │        └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  ├──┬ 10
            │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10
            │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
            │  │  │  ├──┬ input_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.input_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  ├──┬ mlp
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.mlp
            │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
            │  │  │  │  ├──┬ down_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.down_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([960, 2560]) [torch.bfloat16]
            │  │  │  │  ├──┬ gate_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.gate_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  │  └──┬ up_proj:
            │  │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.up_proj
            │  │  │  │     ├─ children: ['weight']
            │  │  │  │     └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  ├──┬ post_attention_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.post_attention_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  └──┬ self_attn:
            │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn
            │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
            │  │  │     ├──┬ k_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.k_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  │     ├──┬ o_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.o_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     ├──┬ q_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.q_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     └──┬ v_proj:
            │  │  │        ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.v_proj
            │  │  │        ├─ children: ['weight']
            │  │  │        └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  ├──┬ 11
            │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11
            │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
            │  │  │  ├──┬ input_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.input_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  ├──┬ mlp
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.mlp
            │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
            │  │  │  │  ├──┬ down_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.down_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([960, 2560]) [torch.bfloat16]
            │  │  │  │  ├──┬ gate_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.gate_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  │  └──┬ up_proj:
            │  │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.up_proj
            │  │  │  │     ├─ children: ['weight']
            │  │  │  │     └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  ├──┬ post_attention_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.post_attention_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  └──┬ self_attn:
            │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn
            │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
            │  │  │     ├──┬ k_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.k_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  │     ├──┬ o_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.o_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     ├──┬ q_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.q_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     └──┬ v_proj:
            │  │  │        ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.v_proj
            │  │  │        ├─ children: ['weight']
            │  │  │        └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  ├──┬ 12
            │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12
            │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
            │  │  │  ├──┬ input_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.input_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  ├──┬ mlp
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.mlp
            │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
            │  │  │  │  ├──┬ down_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.down_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([960, 2560]) [torch.bfloat16]
            │  │  │  │  ├──┬ gate_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.gate_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  │  └──┬ up_proj:
            │  │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.up_proj
            │  │  │  │     ├─ children: ['weight']
            │  │  │  │     └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  ├──┬ post_attention_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.post_attention_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  └──┬ self_attn:
            │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn
            │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
            │  │  │     ├──┬ k_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.k_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  │     ├──┬ o_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.o_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     ├──┬ q_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.q_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     └──┬ v_proj:
            │  │  │        ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.v_proj
            │  │  │        ├─ children: ['weight']
            │  │  │        └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  ├──┬ 13
            │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13
            │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
            │  │  │  ├──┬ input_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.input_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  ├──┬ mlp
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.mlp
            │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
            │  │  │  │  ├──┬ down_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.down_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([960, 2560]) [torch.bfloat16]
            │  │  │  │  ├──┬ gate_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.gate_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  │  └──┬ up_proj:
            │  │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.up_proj
            │  │  │  │     ├─ children: ['weight']
            │  │  │  │     └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  ├──┬ post_attention_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.post_attention_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  └──┬ self_attn:
            │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn
            │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
            │  │  │     ├──┬ k_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.k_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  │     ├──┬ o_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.o_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     ├──┬ q_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.q_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     └──┬ v_proj:
            │  │  │        ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.v_proj
            │  │  │        ├─ children: ['weight']
            │  │  │        └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  ├──┬ 14
            │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14
            │  │  |  ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
            │  │  │  ├──┬ input_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.input_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  ├──┬ mlp
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.mlp
            │  │  │  |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
            │  │  │  │  ├──┬ down_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.down_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([960, 2560]) [torch.bfloat16]
            │  │  │  │  ├──┬ gate_proj
            │  │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.gate_proj
            │  │  │  │  |  ├─ children: ['weight']
            │  │  │  │  │  └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  │  └──┬ up_proj:
            │  │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.up_proj
            │  │  │  │     ├─ children: ['weight']
            │  │  │  │     └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │  │  ├──┬ post_attention_layernorm
            │  │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.post_attention_layernorm
            │  │  │  |  ├─ children: ['weight']
            │  │  │  │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │  │  └──┬ self_attn:
            │  │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn
            │  │  │     ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
            │  │  │     ├──┬ k_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.k_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  │     ├──┬ o_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.o_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     ├──┬ q_proj
            │  │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.q_proj
            │  │  │     |  ├─ children: ['weight']
            │  │  │     │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │  │     └──┬ v_proj:
            │  │  │        ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.v_proj
            │  │  │        ├─ children: ['weight']
            │  │  │        └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │  └──┬ 15:
            │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15
            │  │     ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']
            │  │     ├──┬ input_layernorm
            │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.input_layernorm
            │  │     |  ├─ children: ['weight']
            │  │     │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │     ├──┬ mlp
            │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.mlp
            │  │     |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']
            │  │     │  ├──┬ down_proj
            │  │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.down_proj
            │  │     │  |  ├─ children: ['weight']
            │  │     │  │  └── weight: torch.Size([960, 2560]) [torch.bfloat16]
            │  │     │  ├──┬ gate_proj
            │  │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.gate_proj
            │  │     │  |  ├─ children: ['weight']
            │  │     │  │  └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │     │  └──┬ up_proj:
            │  │     │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.up_proj
            │  │     │     ├─ children: ['weight']
            │  │     │     └── weight: torch.Size([2560, 960]) [torch.bfloat16]
            │  │     ├──┬ post_attention_layernorm
            │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.post_attention_layernorm
            │  │     |  ├─ children: ['weight']
            │  │     │  └── weight: torch.Size([960]) [torch.bfloat16]
            │  │     └──┬ self_attn:
            │  │        ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn
            │  │        ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
            │  │        ├──┬ k_proj
            │  │        |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.k_proj
            │  │        |  ├─ children: ['weight']
            │  │        │  └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  │        ├──┬ o_proj
            │  │        |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.o_proj
            │  │        |  ├─ children: ['weight']
            │  │        │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │        ├──┬ q_proj
            │  │        |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.q_proj
            │  │        |  ├─ children: ['weight']
            │  │        │  └── weight: torch.Size([960, 960]) [torch.bfloat16]
            │  │        └──┬ v_proj:
            │  │           ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.v_proj
            │  │           ├─ children: ['weight']
            │  │           └── weight: torch.Size([320, 960]) [torch.bfloat16]
            │  └──┬ norm:
            │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.norm
            │     ├─ children: ['weight']
            │     └── weight: torch.Size([960]) [torch.bfloat16]
            └──┬ vision_model:
               ├─ in: .model.vlm_with_expert.vlm.model.vision_model
               ├─ children: ['embeddings', 'encoder', 'post_layernorm']
               ├──┬ embeddings
               |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.embeddings
               |  ├─ children: ['patch_embedding', 'position_embedding']
               │  ├──┬ patch_embedding
               │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.embeddings.patch_embedding
               │  |  ├─ children: ['bias', 'weight']
               │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │  │  └── weight: torch.Size([768, 3, 16, 16]) [torch.bfloat16]
               │  └──┬ position_embedding:
               │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.embeddings.position_embedding
               │     ├─ children: ['weight']
               │     └── weight: torch.Size([1024, 768]) [torch.bfloat16]
               ├──┬ encoder
               |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder
               |  ├─ children: ['layers']
               │  └──┬ layers:
               │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers
               │     ├─ children: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
               │     ├──┬ 0
               │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0
               │     |  ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']
               │     │  ├──┬ layer_norm1
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm1
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ layer_norm2
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm2
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ mlp
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp
               │     │  |  ├─ children: ['fc1', 'fc2']
               │     │  │  ├──┬ fc1
               │     │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc1
               │     │  │  |  ├─ children: ['bias', 'weight']
               │     │  │  │  ├── bias: torch.Size([3072]) [torch.bfloat16]
               │     │  │  │  └── weight: torch.Size([3072, 768]) [torch.bfloat16]
               │     │  │  └──┬ fc2:
               │     │  │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc2
               │     │  │     ├─ children: ['bias', 'weight']
               │     │  │     ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │     └── weight: torch.Size([768, 3072]) [torch.bfloat16]
               │     │  └──┬ self_attn:
               │     │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn
               │     │     ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']
               │     │     ├──┬ k_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.k_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ out_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.out_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ q_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.q_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     └──┬ v_proj:
               │     │        ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.v_proj
               │     │        ├─ children: ['bias', 'weight']
               │     │        ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │        └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     ├──┬ 1
               │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1
               │     |  ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']
               │     │  ├──┬ layer_norm1
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm1
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ layer_norm2
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm2
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ mlp
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp
               │     │  |  ├─ children: ['fc1', 'fc2']
               │     │  │  ├──┬ fc1
               │     │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc1
               │     │  │  |  ├─ children: ['bias', 'weight']
               │     │  │  │  ├── bias: torch.Size([3072]) [torch.bfloat16]
               │     │  │  │  └── weight: torch.Size([3072, 768]) [torch.bfloat16]
               │     │  │  └──┬ fc2:
               │     │  │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc2
               │     │  │     ├─ children: ['bias', 'weight']
               │     │  │     ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │     └── weight: torch.Size([768, 3072]) [torch.bfloat16]
               │     │  └──┬ self_attn:
               │     │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn
               │     │     ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']
               │     │     ├──┬ k_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.k_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ out_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.out_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ q_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.q_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     └──┬ v_proj:
               │     │        ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.v_proj
               │     │        ├─ children: ['bias', 'weight']
               │     │        ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │        └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     ├──┬ 2
               │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2
               │     |  ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']
               │     │  ├──┬ layer_norm1
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm1
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ layer_norm2
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm2
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ mlp
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp
               │     │  |  ├─ children: ['fc1', 'fc2']
               │     │  │  ├──┬ fc1
               │     │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc1
               │     │  │  |  ├─ children: ['bias', 'weight']
               │     │  │  │  ├── bias: torch.Size([3072]) [torch.bfloat16]
               │     │  │  │  └── weight: torch.Size([3072, 768]) [torch.bfloat16]
               │     │  │  └──┬ fc2:
               │     │  │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc2
               │     │  │     ├─ children: ['bias', 'weight']
               │     │  │     ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │     └── weight: torch.Size([768, 3072]) [torch.bfloat16]
               │     │  └──┬ self_attn:
               │     │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn
               │     │     ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']
               │     │     ├──┬ k_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.k_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ out_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.out_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ q_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.q_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     └──┬ v_proj:
               │     │        ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.v_proj
               │     │        ├─ children: ['bias', 'weight']
               │     │        ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │        └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     ├──┬ 3
               │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3
               │     |  ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']
               │     │  ├──┬ layer_norm1
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm1
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ layer_norm2
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm2
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ mlp
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp
               │     │  |  ├─ children: ['fc1', 'fc2']
               │     │  │  ├──┬ fc1
               │     │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc1
               │     │  │  |  ├─ children: ['bias', 'weight']
               │     │  │  │  ├── bias: torch.Size([3072]) [torch.bfloat16]
               │     │  │  │  └── weight: torch.Size([3072, 768]) [torch.bfloat16]
               │     │  │  └──┬ fc2:
               │     │  │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc2
               │     │  │     ├─ children: ['bias', 'weight']
               │     │  │     ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │     └── weight: torch.Size([768, 3072]) [torch.bfloat16]
               │     │  └──┬ self_attn:
               │     │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn
               │     │     ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']
               │     │     ├──┬ k_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.k_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ out_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.out_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ q_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.q_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     └──┬ v_proj:
               │     │        ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.v_proj
               │     │        ├─ children: ['bias', 'weight']
               │     │        ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │        └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     ├──┬ 4
               │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4
               │     |  ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']
               │     │  ├──┬ layer_norm1
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm1
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ layer_norm2
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm2
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ mlp
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp
               │     │  |  ├─ children: ['fc1', 'fc2']
               │     │  │  ├──┬ fc1
               │     │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc1
               │     │  │  |  ├─ children: ['bias', 'weight']
               │     │  │  │  ├── bias: torch.Size([3072]) [torch.bfloat16]
               │     │  │  │  └── weight: torch.Size([3072, 768]) [torch.bfloat16]
               │     │  │  └──┬ fc2:
               │     │  │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc2
               │     │  │     ├─ children: ['bias', 'weight']
               │     │  │     ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │     └── weight: torch.Size([768, 3072]) [torch.bfloat16]
               │     │  └──┬ self_attn:
               │     │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn
               │     │     ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']
               │     │     ├──┬ k_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.k_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ out_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.out_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ q_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.q_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     └──┬ v_proj:
               │     │        ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.v_proj
               │     │        ├─ children: ['bias', 'weight']
               │     │        ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │        └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     ├──┬ 5
               │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5
               │     |  ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']
               │     │  ├──┬ layer_norm1
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm1
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ layer_norm2
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm2
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ mlp
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp
               │     │  |  ├─ children: ['fc1', 'fc2']
               │     │  │  ├──┬ fc1
               │     │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc1
               │     │  │  |  ├─ children: ['bias', 'weight']
               │     │  │  │  ├── bias: torch.Size([3072]) [torch.bfloat16]
               │     │  │  │  └── weight: torch.Size([3072, 768]) [torch.bfloat16]
               │     │  │  └──┬ fc2:
               │     │  │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc2
               │     │  │     ├─ children: ['bias', 'weight']
               │     │  │     ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │     └── weight: torch.Size([768, 3072]) [torch.bfloat16]
               │     │  └──┬ self_attn:
               │     │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn
               │     │     ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']
               │     │     ├──┬ k_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.k_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ out_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.out_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ q_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.q_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     └──┬ v_proj:
               │     │        ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.v_proj
               │     │        ├─ children: ['bias', 'weight']
               │     │        ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │        └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     ├──┬ 6
               │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6
               │     |  ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']
               │     │  ├──┬ layer_norm1
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm1
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ layer_norm2
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm2
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ mlp
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp
               │     │  |  ├─ children: ['fc1', 'fc2']
               │     │  │  ├──┬ fc1
               │     │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc1
               │     │  │  |  ├─ children: ['bias', 'weight']
               │     │  │  │  ├── bias: torch.Size([3072]) [torch.bfloat16]
               │     │  │  │  └── weight: torch.Size([3072, 768]) [torch.bfloat16]
               │     │  │  └──┬ fc2:
               │     │  │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc2
               │     │  │     ├─ children: ['bias', 'weight']
               │     │  │     ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │     └── weight: torch.Size([768, 3072]) [torch.bfloat16]
               │     │  └──┬ self_attn:
               │     │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn
               │     │     ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']
               │     │     ├──┬ k_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.k_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ out_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.out_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ q_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.q_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     └──┬ v_proj:
               │     │        ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.v_proj
               │     │        ├─ children: ['bias', 'weight']
               │     │        ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │        └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     ├──┬ 7
               │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7
               │     |  ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']
               │     │  ├──┬ layer_norm1
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm1
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ layer_norm2
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm2
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ mlp
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp
               │     │  |  ├─ children: ['fc1', 'fc2']
               │     │  │  ├──┬ fc1
               │     │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc1
               │     │  │  |  ├─ children: ['bias', 'weight']
               │     │  │  │  ├── bias: torch.Size([3072]) [torch.bfloat16]
               │     │  │  │  └── weight: torch.Size([3072, 768]) [torch.bfloat16]
               │     │  │  └──┬ fc2:
               │     │  │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc2
               │     │  │     ├─ children: ['bias', 'weight']
               │     │  │     ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │     └── weight: torch.Size([768, 3072]) [torch.bfloat16]
               │     │  └──┬ self_attn:
               │     │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn
               │     │     ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']
               │     │     ├──┬ k_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.k_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ out_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.out_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ q_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.q_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     └──┬ v_proj:
               │     │        ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.v_proj
               │     │        ├─ children: ['bias', 'weight']
               │     │        ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │        └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     ├──┬ 8
               │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8
               │     |  ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']
               │     │  ├──┬ layer_norm1
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm1
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ layer_norm2
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm2
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ mlp
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp
               │     │  |  ├─ children: ['fc1', 'fc2']
               │     │  │  ├──┬ fc1
               │     │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc1
               │     │  │  |  ├─ children: ['bias', 'weight']
               │     │  │  │  ├── bias: torch.Size([3072]) [torch.bfloat16]
               │     │  │  │  └── weight: torch.Size([3072, 768]) [torch.bfloat16]
               │     │  │  └──┬ fc2:
               │     │  │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc2
               │     │  │     ├─ children: ['bias', 'weight']
               │     │  │     ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │     └── weight: torch.Size([768, 3072]) [torch.bfloat16]
               │     │  └──┬ self_attn:
               │     │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn
               │     │     ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']
               │     │     ├──┬ k_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.k_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ out_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.out_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ q_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.q_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     └──┬ v_proj:
               │     │        ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.v_proj
               │     │        ├─ children: ['bias', 'weight']
               │     │        ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │        └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     ├──┬ 9
               │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9
               │     |  ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']
               │     │  ├──┬ layer_norm1
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm1
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ layer_norm2
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm2
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ mlp
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp
               │     │  |  ├─ children: ['fc1', 'fc2']
               │     │  │  ├──┬ fc1
               │     │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc1
               │     │  │  |  ├─ children: ['bias', 'weight']
               │     │  │  │  ├── bias: torch.Size([3072]) [torch.bfloat16]
               │     │  │  │  └── weight: torch.Size([3072, 768]) [torch.bfloat16]
               │     │  │  └──┬ fc2:
               │     │  │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc2
               │     │  │     ├─ children: ['bias', 'weight']
               │     │  │     ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │     └── weight: torch.Size([768, 3072]) [torch.bfloat16]
               │     │  └──┬ self_attn:
               │     │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn
               │     │     ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']
               │     │     ├──┬ k_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.k_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ out_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.out_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ q_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.q_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     └──┬ v_proj:
               │     │        ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.v_proj
               │     │        ├─ children: ['bias', 'weight']
               │     │        ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │        └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     ├──┬ 10
               │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10
               │     |  ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']
               │     │  ├──┬ layer_norm1
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm1
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ layer_norm2
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm2
               │     │  |  ├─ children: ['bias', 'weight']
               │     │  │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │  └── weight: torch.Size([768]) [torch.bfloat16]
               │     │  ├──┬ mlp
               │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp
               │     │  |  ├─ children: ['fc1', 'fc2']
               │     │  │  ├──┬ fc1
               │     │  │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc1
               │     │  │  |  ├─ children: ['bias', 'weight']
               │     │  │  │  ├── bias: torch.Size([3072]) [torch.bfloat16]
               │     │  │  │  └── weight: torch.Size([3072, 768]) [torch.bfloat16]
               │     │  │  └──┬ fc2:
               │     │  │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc2
               │     │  │     ├─ children: ['bias', 'weight']
               │     │  │     ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │  │     └── weight: torch.Size([768, 3072]) [torch.bfloat16]
               │     │  └──┬ self_attn:
               │     │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn
               │     │     ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']
               │     │     ├──┬ k_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.k_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ out_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.out_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     ├──┬ q_proj
               │     │     |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.q_proj
               │     │     |  ├─ children: ['bias', 'weight']
               │     │     │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │     │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     │     └──┬ v_proj:
               │     │        ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.v_proj
               │     │        ├─ children: ['bias', 'weight']
               │     │        ├── bias: torch.Size([768]) [torch.bfloat16]
               │     │        └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │     └──┬ 11:
               │        ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11
               │        ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']
               │        ├──┬ layer_norm1
               │        |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm1
               │        |  ├─ children: ['bias', 'weight']
               │        │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │        │  └── weight: torch.Size([768]) [torch.bfloat16]
               │        ├──┬ layer_norm2
               │        |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm2
               │        |  ├─ children: ['bias', 'weight']
               │        │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │        │  └── weight: torch.Size([768]) [torch.bfloat16]
               │        ├──┬ mlp
               │        |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp
               │        |  ├─ children: ['fc1', 'fc2']
               │        │  ├──┬ fc1
               │        │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc1
               │        │  |  ├─ children: ['bias', 'weight']
               │        │  │  ├── bias: torch.Size([3072]) [torch.bfloat16]
               │        │  │  └── weight: torch.Size([3072, 768]) [torch.bfloat16]
               │        │  └──┬ fc2:
               │        │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc2
               │        │     ├─ children: ['bias', 'weight']
               │        │     ├── bias: torch.Size([768]) [torch.bfloat16]
               │        │     └── weight: torch.Size([768, 3072]) [torch.bfloat16]
               │        └──┬ self_attn:
               │           ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn
               │           ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']
               │           ├──┬ k_proj
               │           |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.k_proj
               │           |  ├─ children: ['bias', 'weight']
               │           │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │           │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │           ├──┬ out_proj
               │           |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.out_proj
               │           |  ├─ children: ['bias', 'weight']
               │           │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │           │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │           ├──┬ q_proj
               │           |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.q_proj
               │           |  ├─ children: ['bias', 'weight']
               │           │  ├── bias: torch.Size([768]) [torch.bfloat16]
               │           │  └── weight: torch.Size([768, 768]) [torch.bfloat16]
               │           └──┬ v_proj:
               │              ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.v_proj
               │              ├─ children: ['bias', 'weight']
               │              ├── bias: torch.Size([768]) [torch.bfloat16]
               │              └── weight: torch.Size([768, 768]) [torch.bfloat16]
               └──┬ post_layernorm:
                  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.post_layernorm
                  ├─ children: ['bias', 'weight']
                  ├── bias: torch.Size([768]) [torch.bfloat16]
                  └── weight: torch.Size([768]) [torch.bfloat16]
