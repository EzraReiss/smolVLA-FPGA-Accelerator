├───┬ input_layernorm
|   ├─ in: model.vlm_with_expert.lm_expert.layers.2.input_layernorm
|   ├─ children: ['weight']
│   └──── weight: torch.Size([720]) [torch.bfloat16]
├───┬ mlp
|   ├─ in: model.vlm_with_expert.lm_expert.layers.2.mlp
|   ├─ children: ['down_proj', 'gate_proj', 'up_proj']
│   ├───┬ down_proj
│   |   ├─ in: model.vlm_with_expert.lm_expert.layers.2.mlp.down_proj
│   |   ├─ children: ['weight']
│   │   └──── weight: torch.Size([720, 2048]) [torch.bfloat16]
│   ├───┬ gate_proj
│   |   ├─ in: model.vlm_with_expert.lm_expert.layers.2.mlp.gate_proj
│   |   ├─ children: ['weight']
│   │   └──── weight: torch.Size([2048, 720]) [torch.bfloat16]
│   └───┬ up_proj:
│        ├─ in: model.vlm_with_expert.lm_expert.layers.2.mlp.up_proj
│       ├─ children: ['weight']
│       └──── weight: torch.Size([2048, 720]) [torch.bfloat16]
├───┬ post_attention_layernorm
|   ├─ in: model.vlm_with_expert.lm_expert.layers.2.post_attention_layernorm
|   ├─ children: ['weight']
│   └──── weight: torch.Size([720]) [torch.bfloat16]
└───┬ self_attn:
     ├─ in: model.vlm_with_expert.lm_expert.layers.2.self_attn
    ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']
    ├───┬ k_proj
    |   ├─ in: model.vlm_with_expert.lm_expert.layers.2.self_attn.k_proj
    |   ├─ children: ['weight']
    │   └──── weight: torch.Size([320, 720]) [torch.bfloat16]
    ├───┬ o_proj
    |   ├─ in: model.vlm_with_expert.lm_expert.layers.2.self_attn.o_proj
    |   ├─ children: ['weight']
    │   └──── weight: torch.Size([720, 960]) [torch.bfloat16]
    ├───┬ q_proj
    |   ├─ in: model.vlm_with_expert.lm_expert.layers.2.self_attn.q_proj
    |   ├─ children: ['weight']
    │   └──── weight: torch.Size([960, 720]) [torch.bfloat16]
    └───┬ v_proj:
         ├─ in: model.vlm_with_expert.lm_expert.layers.2.self_attn.v_proj
        ├─ children: ['weight']
        └──── weight: torch.Size([320, 720]) [torch.bfloat16]
