{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10941ba",
   "metadata": {},
   "source": [
    "# Setup\n",
    " - Point to your weights\n",
    " - Pick a tiny input shape for vectors\n",
    " - All outputs go to ./smolvla_test_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5325ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os, json, math, re, pathlib, itertools\n",
    "from typing import Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# <<< EDIT IF NEEDED >>>\n",
    "model_weights = \"../weights/downloads/model.safetensors\"  # <= your path\n",
    "out_dir = \"./smolvla_test_vectors\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff10143",
   "metadata": {},
   "source": [
    "1) Load safetensors and index keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f7b8335",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = load_safetensors(model_weights, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9165f18f",
   "metadata": {},
   "source": [
    "2. Helper functions for printing out and understanding model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa817b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dictionary_tree(dict, split='.'):\n",
    "    tree = {}\n",
    "\n",
    "    for key, value in dict.items():\n",
    "        parts = key.split(split)\n",
    "        tree_key = parts[0]\n",
    "        if len(parts) == 1:\n",
    "            tree[tree_key] = value\n",
    "            continue\n",
    "        else:\n",
    "            subtree_key = split.join(parts[1:])\n",
    "            \n",
    "            if tree_key not in tree:\n",
    "                tree[tree_key] = { subtree_key: value }\n",
    "            else:\n",
    "                tree[tree_key][subtree_key] = value\n",
    "\n",
    "    \n",
    "    # if all items are numbers, sort them by numeric order\n",
    "    if all(re.match(r'^\\d+$', str(k)) for k, v in tree.items()):\n",
    "        tree = { k: v for k, v in sorted(tree.items(), key=lambda x: int(x[0])) }\n",
    "\n",
    "    for key in tree.keys():\n",
    "        if isinstance(tree[key], Dict):\n",
    "            tree[key] = make_dictionary_tree(tree[key], split=split)    \n",
    "\n",
    "    return tree\n",
    "\n",
    "def dictionary_tree_as_string(tree, prefix='', key_prefix=''):\n",
    "    items = list(tree.items())\n",
    "    result = \"\"\n",
    "\n",
    "    for key, value in items[:-1]:\n",
    "        if isinstance(value, dict):\n",
    "            result += f\"{prefix}├───┬ {key}\\n\"\n",
    "            if key_prefix:\n",
    "                result += f\"{prefix}|   ├─ in: {key_prefix}.{key}\\n\"\n",
    "            if isinstance(value, dict):\n",
    "                result += f\"{prefix}|   ├─ children: {list(value.keys())}\\n\"\n",
    "            result += dictionary_tree_as_string(\n",
    "                value, \n",
    "                f\"{prefix}│   \",\n",
    "                f\"{key_prefix}.{key}\"\n",
    "            )\n",
    "        else:\n",
    "            result += f\"{prefix}├──── {key}: {value.shape} [{value.dtype}]\\n\"\n",
    "\n",
    "    last_key, last_value = items[-1]\n",
    "    if isinstance(last_value, dict):\n",
    "        result += f\"{prefix}└───┬ {last_key}:\\n\"\n",
    "        if key_prefix:\n",
    "            result += f\"{prefix}    ├─ in: {key_prefix}.{last_key}\\n\"\n",
    "        if isinstance(last_value, dict):\n",
    "            result += f\"{prefix}    ├─ children: {list(last_value.keys())}\\n\"\n",
    "        result += dictionary_tree_as_string(\n",
    "            last_value, \n",
    "            f\"{prefix}    \",\n",
    "            f\"{key_prefix}.{last_key}\"\n",
    "        )\n",
    "    else:\n",
    "        result += f\"{prefix}└──── {last_key}: {last_value.shape} [{last_value.dtype}]\\n\"\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c23ef8f",
   "metadata": {},
   "source": [
    "3. prints out the actual weights as a nice dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1da5b001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "└───┬ model:\n",
      "    ├─ children: ['action_in_proj', 'action_out_proj', 'action_time_mlp_in', 'action_time_mlp_out', 'state_proj', 'vlm_with_expert']\n",
      "    ├───┬ action_in_proj\n",
      "    |   ├─ in: .model.action_in_proj\n",
      "    |   ├─ children: ['bias', 'weight']\n",
      "    │   ├──── bias: torch.Size([720]) [torch.float32]\n",
      "    │   └──── weight: torch.Size([720, 32]) [torch.float32]\n",
      "    ├───┬ action_out_proj\n",
      "    |   ├─ in: .model.action_out_proj\n",
      "    |   ├─ children: ['bias', 'weight']\n",
      "    │   ├──── bias: torch.Size([32]) [torch.float32]\n",
      "    │   └──── weight: torch.Size([32, 720]) [torch.float32]\n",
      "    ├───┬ action_time_mlp_in\n",
      "    |   ├─ in: .model.action_time_mlp_in\n",
      "    |   ├─ children: ['bias', 'weight']\n",
      "    │   ├──── bias: torch.Size([720]) [torch.float32]\n",
      "    │   └──── weight: torch.Size([720, 1440]) [torch.float32]\n",
      "    ├───┬ action_time_mlp_out\n",
      "    |   ├─ in: .model.action_time_mlp_out\n",
      "    |   ├─ children: ['bias', 'weight']\n",
      "    │   ├──── bias: torch.Size([720]) [torch.float32]\n",
      "    │   └──── weight: torch.Size([720, 720]) [torch.float32]\n",
      "    ├───┬ state_proj\n",
      "    |   ├─ in: .model.state_proj\n",
      "    |   ├─ children: ['bias', 'weight']\n",
      "    │   ├──── bias: torch.Size([960]) [torch.float32]\n",
      "    │   └──── weight: torch.Size([960, 32]) [torch.float32]\n",
      "    └───┬ vlm_with_expert:\n",
      "        ├─ in: .model.vlm_with_expert\n",
      "        ├─ children: ['lm_expert', 'vlm']\n",
      "        ├───┬ lm_expert\n",
      "        |   ├─ in: .model.vlm_with_expert.lm_expert\n",
      "        |   ├─ children: ['layers', 'norm']\n",
      "        │   ├───┬ layers\n",
      "        │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers\n",
      "        │   |   ├─ children: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15']\n",
      "        │   │   ├───┬ 0\n",
      "        │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.0\n",
      "        │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "        │   │   │   ├───┬ input_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.0.input_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ mlp\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.0.mlp\n",
      "        │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "        │   │   │   │   ├───┬ down_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.0.mlp.down_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 2048]) [torch.bfloat16]\n",
      "        │   │   │   │   ├───┬ gate_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.0.mlp.gate_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ up_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.0.mlp.up_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ post_attention_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.0.post_attention_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   └───┬ self_attn:\n",
      "        │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.0.self_attn\n",
      "        │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "        │   │   │       ├───┬ k_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.0.self_attn.k_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([320, 720]) [torch.bfloat16]\n",
      "        │   │   │       ├───┬ o_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.0.self_attn.o_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([720, 960]) [torch.bfloat16]\n",
      "        │   │   │       ├───┬ q_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.0.self_attn.q_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([960, 720]) [torch.bfloat16]\n",
      "        │   │   │       └───┬ v_proj:\n",
      "        │   │   │           ├─ in: .model.vlm_with_expert.lm_expert.layers.0.self_attn.v_proj\n",
      "        │   │   │           ├─ children: ['weight']\n",
      "        │   │   │           └──── weight: torch.Size([320, 720]) [torch.bfloat16]\n",
      "        │   │   ├───┬ 1\n",
      "        │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.1\n",
      "        │   │   |   ├─ children: ['self_attn', 'input_layernorm', 'mlp', 'post_attention_layernorm']\n",
      "        │   │   │   ├───┬ self_attn\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.1.self_attn\n",
      "        │   │   │   |   ├─ children: ['k_proj', 'v_proj', 'o_proj', 'q_proj']\n",
      "        │   │   │   │   ├───┬ k_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.1.self_attn.k_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([320, 320]) [torch.float32]\n",
      "        │   │   │   │   ├───┬ v_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.1.self_attn.v_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([320, 320]) [torch.float32]\n",
      "        │   │   │   │   ├───┬ o_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.1.self_attn.o_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 960]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ q_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.1.self_attn.q_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([960, 720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ input_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.1.input_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ mlp\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.1.mlp\n",
      "        │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "        │   │   │   │   ├───┬ down_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.1.mlp.down_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 2048]) [torch.bfloat16]\n",
      "        │   │   │   │   ├───┬ gate_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.1.mlp.gate_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ up_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.1.mlp.up_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   └───┬ post_attention_layernorm:\n",
      "        │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.1.post_attention_layernorm\n",
      "        │   │   │       ├─ children: ['weight']\n",
      "        │   │   │       └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   ├───┬ 2\n",
      "        │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.2\n",
      "        │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "        │   │   │   ├───┬ input_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.2.input_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ mlp\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.2.mlp\n",
      "        │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "        │   │   │   │   ├───┬ down_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.2.mlp.down_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 2048]) [torch.bfloat16]\n",
      "        │   │   │   │   ├───┬ gate_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.2.mlp.gate_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ up_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.2.mlp.up_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ post_attention_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.2.post_attention_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   └───┬ self_attn:\n",
      "        │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.2.self_attn\n",
      "        │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "        │   │   │       ├───┬ k_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.2.self_attn.k_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([320, 720]) [torch.bfloat16]\n",
      "        │   │   │       ├───┬ o_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.2.self_attn.o_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([720, 960]) [torch.bfloat16]\n",
      "        │   │   │       ├───┬ q_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.2.self_attn.q_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([960, 720]) [torch.bfloat16]\n",
      "        │   │   │       └───┬ v_proj:\n",
      "        │   │   │           ├─ in: .model.vlm_with_expert.lm_expert.layers.2.self_attn.v_proj\n",
      "        │   │   │           ├─ children: ['weight']\n",
      "        │   │   │           └──── weight: torch.Size([320, 720]) [torch.bfloat16]\n",
      "        │   │   ├───┬ 3\n",
      "        │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.3\n",
      "        │   │   |   ├─ children: ['self_attn', 'input_layernorm', 'mlp', 'post_attention_layernorm']\n",
      "        │   │   │   ├───┬ self_attn\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.3.self_attn\n",
      "        │   │   │   |   ├─ children: ['k_proj', 'v_proj', 'o_proj', 'q_proj']\n",
      "        │   │   │   │   ├───┬ k_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.3.self_attn.k_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([320, 320]) [torch.float32]\n",
      "        │   │   │   │   ├───┬ v_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.3.self_attn.v_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([320, 320]) [torch.float32]\n",
      "        │   │   │   │   ├───┬ o_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.3.self_attn.o_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 960]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ q_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.3.self_attn.q_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([960, 720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ input_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.3.input_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ mlp\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.3.mlp\n",
      "        │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "        │   │   │   │   ├───┬ down_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.3.mlp.down_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 2048]) [torch.bfloat16]\n",
      "        │   │   │   │   ├───┬ gate_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.3.mlp.gate_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ up_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.3.mlp.up_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   └───┬ post_attention_layernorm:\n",
      "        │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.3.post_attention_layernorm\n",
      "        │   │   │       ├─ children: ['weight']\n",
      "        │   │   │       └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   ├───┬ 4\n",
      "        │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.4\n",
      "        │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "        │   │   │   ├───┬ input_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.4.input_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ mlp\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.4.mlp\n",
      "        │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "        │   │   │   │   ├───┬ down_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.4.mlp.down_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 2048]) [torch.bfloat16]\n",
      "        │   │   │   │   ├───┬ gate_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.4.mlp.gate_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ up_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.4.mlp.up_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ post_attention_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.4.post_attention_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   └───┬ self_attn:\n",
      "        │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.4.self_attn\n",
      "        │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "        │   │   │       ├───┬ k_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.4.self_attn.k_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([320, 720]) [torch.bfloat16]\n",
      "        │   │   │       ├───┬ o_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.4.self_attn.o_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([720, 960]) [torch.bfloat16]\n",
      "        │   │   │       ├───┬ q_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.4.self_attn.q_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([960, 720]) [torch.bfloat16]\n",
      "        │   │   │       └───┬ v_proj:\n",
      "        │   │   │           ├─ in: .model.vlm_with_expert.lm_expert.layers.4.self_attn.v_proj\n",
      "        │   │   │           ├─ children: ['weight']\n",
      "        │   │   │           └──── weight: torch.Size([320, 720]) [torch.bfloat16]\n",
      "        │   │   ├───┬ 5\n",
      "        │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.5\n",
      "        │   │   |   ├─ children: ['self_attn', 'input_layernorm', 'mlp', 'post_attention_layernorm']\n",
      "        │   │   │   ├───┬ self_attn\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.5.self_attn\n",
      "        │   │   │   |   ├─ children: ['k_proj', 'v_proj', 'o_proj', 'q_proj']\n",
      "        │   │   │   │   ├───┬ k_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.5.self_attn.k_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([320, 320]) [torch.float32]\n",
      "        │   │   │   │   ├───┬ v_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.5.self_attn.v_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([320, 320]) [torch.float32]\n",
      "        │   │   │   │   ├───┬ o_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.5.self_attn.o_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 960]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ q_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.5.self_attn.q_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([960, 720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ input_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.5.input_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ mlp\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.5.mlp\n",
      "        │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "        │   │   │   │   ├───┬ down_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.5.mlp.down_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 2048]) [torch.bfloat16]\n",
      "        │   │   │   │   ├───┬ gate_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.5.mlp.gate_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ up_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.5.mlp.up_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   └───┬ post_attention_layernorm:\n",
      "        │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.5.post_attention_layernorm\n",
      "        │   │   │       ├─ children: ['weight']\n",
      "        │   │   │       └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   ├───┬ 6\n",
      "        │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.6\n",
      "        │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "        │   │   │   ├───┬ input_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.6.input_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ mlp\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.6.mlp\n",
      "        │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "        │   │   │   │   ├───┬ down_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.6.mlp.down_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 2048]) [torch.bfloat16]\n",
      "        │   │   │   │   ├───┬ gate_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.6.mlp.gate_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ up_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.6.mlp.up_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ post_attention_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.6.post_attention_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   └───┬ self_attn:\n",
      "        │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.6.self_attn\n",
      "        │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "        │   │   │       ├───┬ k_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.6.self_attn.k_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([320, 720]) [torch.bfloat16]\n",
      "        │   │   │       ├───┬ o_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.6.self_attn.o_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([720, 960]) [torch.bfloat16]\n",
      "        │   │   │       ├───┬ q_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.6.self_attn.q_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([960, 720]) [torch.bfloat16]\n",
      "        │   │   │       └───┬ v_proj:\n",
      "        │   │   │           ├─ in: .model.vlm_with_expert.lm_expert.layers.6.self_attn.v_proj\n",
      "        │   │   │           ├─ children: ['weight']\n",
      "        │   │   │           └──── weight: torch.Size([320, 720]) [torch.bfloat16]\n",
      "        │   │   ├───┬ 7\n",
      "        │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.7\n",
      "        │   │   |   ├─ children: ['self_attn', 'input_layernorm', 'mlp', 'post_attention_layernorm']\n",
      "        │   │   │   ├───┬ self_attn\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.7.self_attn\n",
      "        │   │   │   |   ├─ children: ['k_proj', 'v_proj', 'o_proj', 'q_proj']\n",
      "        │   │   │   │   ├───┬ k_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.7.self_attn.k_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([320, 320]) [torch.float32]\n",
      "        │   │   │   │   ├───┬ v_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.7.self_attn.v_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([320, 320]) [torch.float32]\n",
      "        │   │   │   │   ├───┬ o_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.7.self_attn.o_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 960]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ q_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.7.self_attn.q_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([960, 720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ input_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.7.input_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ mlp\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.7.mlp\n",
      "        │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "        │   │   │   │   ├───┬ down_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.7.mlp.down_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 2048]) [torch.bfloat16]\n",
      "        │   │   │   │   ├───┬ gate_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.7.mlp.gate_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ up_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.7.mlp.up_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   └───┬ post_attention_layernorm:\n",
      "        │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.7.post_attention_layernorm\n",
      "        │   │   │       ├─ children: ['weight']\n",
      "        │   │   │       └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   ├───┬ 8\n",
      "        │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.8\n",
      "        │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "        │   │   │   ├───┬ input_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.8.input_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ mlp\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.8.mlp\n",
      "        │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "        │   │   │   │   ├───┬ down_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.8.mlp.down_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 2048]) [torch.bfloat16]\n",
      "        │   │   │   │   ├───┬ gate_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.8.mlp.gate_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ up_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.8.mlp.up_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ post_attention_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.8.post_attention_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   └───┬ self_attn:\n",
      "        │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.8.self_attn\n",
      "        │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "        │   │   │       ├───┬ k_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.8.self_attn.k_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([320, 720]) [torch.bfloat16]\n",
      "        │   │   │       ├───┬ o_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.8.self_attn.o_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([720, 960]) [torch.bfloat16]\n",
      "        │   │   │       ├───┬ q_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.8.self_attn.q_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([960, 720]) [torch.bfloat16]\n",
      "        │   │   │       └───┬ v_proj:\n",
      "        │   │   │           ├─ in: .model.vlm_with_expert.lm_expert.layers.8.self_attn.v_proj\n",
      "        │   │   │           ├─ children: ['weight']\n",
      "        │   │   │           └──── weight: torch.Size([320, 720]) [torch.bfloat16]\n",
      "        │   │   ├───┬ 9\n",
      "        │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.9\n",
      "        │   │   |   ├─ children: ['self_attn', 'input_layernorm', 'mlp', 'post_attention_layernorm']\n",
      "        │   │   │   ├───┬ self_attn\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.9.self_attn\n",
      "        │   │   │   |   ├─ children: ['k_proj', 'v_proj', 'o_proj', 'q_proj']\n",
      "        │   │   │   │   ├───┬ k_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.9.self_attn.k_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([320, 320]) [torch.float32]\n",
      "        │   │   │   │   ├───┬ v_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.9.self_attn.v_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([320, 320]) [torch.float32]\n",
      "        │   │   │   │   ├───┬ o_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.9.self_attn.o_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 960]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ q_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.9.self_attn.q_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([960, 720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ input_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.9.input_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ mlp\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.9.mlp\n",
      "        │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "        │   │   │   │   ├───┬ down_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.9.mlp.down_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 2048]) [torch.bfloat16]\n",
      "        │   │   │   │   ├───┬ gate_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.9.mlp.gate_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ up_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.9.mlp.up_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   └───┬ post_attention_layernorm:\n",
      "        │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.9.post_attention_layernorm\n",
      "        │   │   │       ├─ children: ['weight']\n",
      "        │   │   │       └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   ├───┬ 10\n",
      "        │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.10\n",
      "        │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "        │   │   │   ├───┬ input_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.10.input_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ mlp\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.10.mlp\n",
      "        │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "        │   │   │   │   ├───┬ down_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.10.mlp.down_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 2048]) [torch.bfloat16]\n",
      "        │   │   │   │   ├───┬ gate_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.10.mlp.gate_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ up_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.10.mlp.up_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ post_attention_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.10.post_attention_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   └───┬ self_attn:\n",
      "        │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.10.self_attn\n",
      "        │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "        │   │   │       ├───┬ k_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.10.self_attn.k_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([320, 720]) [torch.bfloat16]\n",
      "        │   │   │       ├───┬ o_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.10.self_attn.o_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([720, 960]) [torch.bfloat16]\n",
      "        │   │   │       ├───┬ q_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.10.self_attn.q_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([960, 720]) [torch.bfloat16]\n",
      "        │   │   │       └───┬ v_proj:\n",
      "        │   │   │           ├─ in: .model.vlm_with_expert.lm_expert.layers.10.self_attn.v_proj\n",
      "        │   │   │           ├─ children: ['weight']\n",
      "        │   │   │           └──── weight: torch.Size([320, 720]) [torch.bfloat16]\n",
      "        │   │   ├───┬ 11\n",
      "        │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.11\n",
      "        │   │   |   ├─ children: ['self_attn', 'input_layernorm', 'mlp', 'post_attention_layernorm']\n",
      "        │   │   │   ├───┬ self_attn\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.11.self_attn\n",
      "        │   │   │   |   ├─ children: ['k_proj', 'v_proj', 'o_proj', 'q_proj']\n",
      "        │   │   │   │   ├───┬ k_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.11.self_attn.k_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([320, 320]) [torch.float32]\n",
      "        │   │   │   │   ├───┬ v_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.11.self_attn.v_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([320, 320]) [torch.float32]\n",
      "        │   │   │   │   ├───┬ o_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.11.self_attn.o_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 960]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ q_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.11.self_attn.q_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([960, 720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ input_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.11.input_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ mlp\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.11.mlp\n",
      "        │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "        │   │   │   │   ├───┬ down_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.11.mlp.down_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 2048]) [torch.bfloat16]\n",
      "        │   │   │   │   ├───┬ gate_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.11.mlp.gate_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ up_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.11.mlp.up_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   └───┬ post_attention_layernorm:\n",
      "        │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.11.post_attention_layernorm\n",
      "        │   │   │       ├─ children: ['weight']\n",
      "        │   │   │       └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   ├───┬ 12\n",
      "        │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.12\n",
      "        │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "        │   │   │   ├───┬ input_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.12.input_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ mlp\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.12.mlp\n",
      "        │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "        │   │   │   │   ├───┬ down_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.12.mlp.down_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 2048]) [torch.bfloat16]\n",
      "        │   │   │   │   ├───┬ gate_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.12.mlp.gate_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ up_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.12.mlp.up_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ post_attention_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.12.post_attention_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   └───┬ self_attn:\n",
      "        │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.12.self_attn\n",
      "        │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "        │   │   │       ├───┬ k_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.12.self_attn.k_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([320, 720]) [torch.bfloat16]\n",
      "        │   │   │       ├───┬ o_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.12.self_attn.o_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([720, 960]) [torch.bfloat16]\n",
      "        │   │   │       ├───┬ q_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.12.self_attn.q_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([960, 720]) [torch.bfloat16]\n",
      "        │   │   │       └───┬ v_proj:\n",
      "        │   │   │           ├─ in: .model.vlm_with_expert.lm_expert.layers.12.self_attn.v_proj\n",
      "        │   │   │           ├─ children: ['weight']\n",
      "        │   │   │           └──── weight: torch.Size([320, 720]) [torch.bfloat16]\n",
      "        │   │   ├───┬ 13\n",
      "        │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.13\n",
      "        │   │   |   ├─ children: ['self_attn', 'input_layernorm', 'mlp', 'post_attention_layernorm']\n",
      "        │   │   │   ├───┬ self_attn\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.13.self_attn\n",
      "        │   │   │   |   ├─ children: ['k_proj', 'v_proj', 'o_proj', 'q_proj']\n",
      "        │   │   │   │   ├───┬ k_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.13.self_attn.k_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([320, 320]) [torch.float32]\n",
      "        │   │   │   │   ├───┬ v_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.13.self_attn.v_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([320, 320]) [torch.float32]\n",
      "        │   │   │   │   ├───┬ o_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.13.self_attn.o_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 960]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ q_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.13.self_attn.q_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([960, 720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ input_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.13.input_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ mlp\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.13.mlp\n",
      "        │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "        │   │   │   │   ├───┬ down_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.13.mlp.down_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 2048]) [torch.bfloat16]\n",
      "        │   │   │   │   ├───┬ gate_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.13.mlp.gate_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ up_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.13.mlp.up_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   └───┬ post_attention_layernorm:\n",
      "        │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.13.post_attention_layernorm\n",
      "        │   │   │       ├─ children: ['weight']\n",
      "        │   │   │       └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   ├───┬ 14\n",
      "        │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.14\n",
      "        │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "        │   │   │   ├───┬ input_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.14.input_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ mlp\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.14.mlp\n",
      "        │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "        │   │   │   │   ├───┬ down_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.14.mlp.down_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([720, 2048]) [torch.bfloat16]\n",
      "        │   │   │   │   ├───┬ gate_proj\n",
      "        │   │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.14.mlp.gate_proj\n",
      "        │   │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   │   └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   │   └───┬ up_proj:\n",
      "        │   │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.14.mlp.up_proj\n",
      "        │   │   │   │       ├─ children: ['weight']\n",
      "        │   │   │   │       └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │   │   ├───┬ post_attention_layernorm\n",
      "        │   │   │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.14.post_attention_layernorm\n",
      "        │   │   │   |   ├─ children: ['weight']\n",
      "        │   │   │   │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │   │   └───┬ self_attn:\n",
      "        │   │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.14.self_attn\n",
      "        │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "        │   │   │       ├───┬ k_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.14.self_attn.k_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([320, 720]) [torch.bfloat16]\n",
      "        │   │   │       ├───┬ o_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.14.self_attn.o_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([720, 960]) [torch.bfloat16]\n",
      "        │   │   │       ├───┬ q_proj\n",
      "        │   │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.14.self_attn.q_proj\n",
      "        │   │   │       |   ├─ children: ['weight']\n",
      "        │   │   │       │   └──── weight: torch.Size([960, 720]) [torch.bfloat16]\n",
      "        │   │   │       └───┬ v_proj:\n",
      "        │   │   │           ├─ in: .model.vlm_with_expert.lm_expert.layers.14.self_attn.v_proj\n",
      "        │   │   │           ├─ children: ['weight']\n",
      "        │   │   │           └──── weight: torch.Size([320, 720]) [torch.bfloat16]\n",
      "        │   │   └───┬ 15:\n",
      "        │   │       ├─ in: .model.vlm_with_expert.lm_expert.layers.15\n",
      "        │   │       ├─ children: ['self_attn', 'input_layernorm', 'mlp', 'post_attention_layernorm']\n",
      "        │   │       ├───┬ self_attn\n",
      "        │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.15.self_attn\n",
      "        │   │       |   ├─ children: ['k_proj', 'v_proj', 'o_proj', 'q_proj']\n",
      "        │   │       │   ├───┬ k_proj\n",
      "        │   │       │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.15.self_attn.k_proj\n",
      "        │   │       │   |   ├─ children: ['weight']\n",
      "        │   │       │   │   └──── weight: torch.Size([320, 320]) [torch.float32]\n",
      "        │   │       │   ├───┬ v_proj\n",
      "        │   │       │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.15.self_attn.v_proj\n",
      "        │   │       │   |   ├─ children: ['weight']\n",
      "        │   │       │   │   └──── weight: torch.Size([320, 320]) [torch.float32]\n",
      "        │   │       │   ├───┬ o_proj\n",
      "        │   │       │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.15.self_attn.o_proj\n",
      "        │   │       │   |   ├─ children: ['weight']\n",
      "        │   │       │   │   └──── weight: torch.Size([720, 960]) [torch.bfloat16]\n",
      "        │   │       │   └───┬ q_proj:\n",
      "        │   │       │       ├─ in: .model.vlm_with_expert.lm_expert.layers.15.self_attn.q_proj\n",
      "        │   │       │       ├─ children: ['weight']\n",
      "        │   │       │       └──── weight: torch.Size([960, 720]) [torch.bfloat16]\n",
      "        │   │       ├───┬ input_layernorm\n",
      "        │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.15.input_layernorm\n",
      "        │   │       |   ├─ children: ['weight']\n",
      "        │   │       │   └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   │       ├───┬ mlp\n",
      "        │   │       |   ├─ in: .model.vlm_with_expert.lm_expert.layers.15.mlp\n",
      "        │   │       |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "        │   │       │   ├───┬ down_proj\n",
      "        │   │       │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.15.mlp.down_proj\n",
      "        │   │       │   |   ├─ children: ['weight']\n",
      "        │   │       │   │   └──── weight: torch.Size([720, 2048]) [torch.bfloat16]\n",
      "        │   │       │   ├───┬ gate_proj\n",
      "        │   │       │   |   ├─ in: .model.vlm_with_expert.lm_expert.layers.15.mlp.gate_proj\n",
      "        │   │       │   |   ├─ children: ['weight']\n",
      "        │   │       │   │   └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │       │   └───┬ up_proj:\n",
      "        │   │       │       ├─ in: .model.vlm_with_expert.lm_expert.layers.15.mlp.up_proj\n",
      "        │   │       │       ├─ children: ['weight']\n",
      "        │   │       │       └──── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "        │   │       └───┬ post_attention_layernorm:\n",
      "        │   │           ├─ in: .model.vlm_with_expert.lm_expert.layers.15.post_attention_layernorm\n",
      "        │   │           ├─ children: ['weight']\n",
      "        │   │           └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        │   └───┬ norm:\n",
      "        │       ├─ in: .model.vlm_with_expert.lm_expert.norm\n",
      "        │       ├─ children: ['weight']\n",
      "        │       └──── weight: torch.Size([720]) [torch.bfloat16]\n",
      "        └───┬ vlm:\n",
      "            ├─ in: .model.vlm_with_expert.vlm\n",
      "            ├─ children: ['lm_head', 'model']\n",
      "            ├───┬ lm_head\n",
      "            |   ├─ in: .model.vlm_with_expert.vlm.lm_head\n",
      "            |   ├─ children: ['weight']\n",
      "            │   └──── weight: torch.Size([49280, 960]) [torch.bfloat16]\n",
      "            └───┬ model:\n",
      "                ├─ in: .model.vlm_with_expert.vlm.model\n",
      "                ├─ children: ['connector', 'text_model', 'vision_model']\n",
      "                ├───┬ connector\n",
      "                |   ├─ in: .model.vlm_with_expert.vlm.model.connector\n",
      "                |   ├─ children: ['modality_projection']\n",
      "                │   └───┬ modality_projection:\n",
      "                │       ├─ in: .model.vlm_with_expert.vlm.model.connector.modality_projection\n",
      "                │       ├─ children: ['proj']\n",
      "                │       └───┬ proj:\n",
      "                │           ├─ in: .model.vlm_with_expert.vlm.model.connector.modality_projection.proj\n",
      "                │           ├─ children: ['weight']\n",
      "                │           └──── weight: torch.Size([960, 12288]) [torch.bfloat16]\n",
      "                ├───┬ text_model\n",
      "                |   ├─ in: .model.vlm_with_expert.vlm.model.text_model\n",
      "                |   ├─ children: ['embed_tokens', 'layers', 'norm']\n",
      "                │   ├───┬ embed_tokens\n",
      "                │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.embed_tokens\n",
      "                │   |   ├─ children: ['weight']\n",
      "                │   │   └──── weight: torch.Size([49280, 960]) [torch.bfloat16]\n",
      "                │   ├───┬ layers\n",
      "                │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers\n",
      "                │   |   ├─ children: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15']\n",
      "                │   │   ├───┬ 0\n",
      "                │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0\n",
      "                │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "                │   │   │   ├───┬ input_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.input_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ mlp\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.mlp\n",
      "                │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "                │   │   │   │   ├───┬ down_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.down_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([960, 2560]) [torch.bfloat16]\n",
      "                │   │   │   │   ├───┬ gate_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.gate_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   │   └───┬ up_proj:\n",
      "                │   │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.up_proj\n",
      "                │   │   │   │       ├─ children: ['weight']\n",
      "                │   │   │   │       └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ post_attention_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.post_attention_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   └───┬ self_attn:\n",
      "                │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn\n",
      "                │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "                │   │   │       ├───┬ k_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.k_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ o_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.o_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ q_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.q_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       └───┬ v_proj:\n",
      "                │   │   │           ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.v_proj\n",
      "                │   │   │           ├─ children: ['weight']\n",
      "                │   │   │           └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   ├───┬ 1\n",
      "                │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1\n",
      "                │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "                │   │   │   ├───┬ input_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.input_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ mlp\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.mlp\n",
      "                │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "                │   │   │   │   ├───┬ down_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.down_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([960, 2560]) [torch.bfloat16]\n",
      "                │   │   │   │   ├───┬ gate_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.gate_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   │   └───┬ up_proj:\n",
      "                │   │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.up_proj\n",
      "                │   │   │   │       ├─ children: ['weight']\n",
      "                │   │   │   │       └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ post_attention_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.post_attention_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   └───┬ self_attn:\n",
      "                │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn\n",
      "                │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "                │   │   │       ├───┬ k_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.k_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ o_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.o_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ q_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.q_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       └───┬ v_proj:\n",
      "                │   │   │           ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.v_proj\n",
      "                │   │   │           ├─ children: ['weight']\n",
      "                │   │   │           └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   ├───┬ 2\n",
      "                │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2\n",
      "                │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "                │   │   │   ├───┬ input_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.input_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ mlp\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.mlp\n",
      "                │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "                │   │   │   │   ├───┬ down_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.down_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([960, 2560]) [torch.bfloat16]\n",
      "                │   │   │   │   ├───┬ gate_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.gate_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   │   └───┬ up_proj:\n",
      "                │   │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.up_proj\n",
      "                │   │   │   │       ├─ children: ['weight']\n",
      "                │   │   │   │       └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ post_attention_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.post_attention_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   └───┬ self_attn:\n",
      "                │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn\n",
      "                │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "                │   │   │       ├───┬ k_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.k_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ o_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.o_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ q_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.q_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       └───┬ v_proj:\n",
      "                │   │   │           ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.v_proj\n",
      "                │   │   │           ├─ children: ['weight']\n",
      "                │   │   │           └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   ├───┬ 3\n",
      "                │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3\n",
      "                │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "                │   │   │   ├───┬ input_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.input_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ mlp\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.mlp\n",
      "                │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "                │   │   │   │   ├───┬ down_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.down_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([960, 2560]) [torch.bfloat16]\n",
      "                │   │   │   │   ├───┬ gate_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.gate_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   │   └───┬ up_proj:\n",
      "                │   │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.up_proj\n",
      "                │   │   │   │       ├─ children: ['weight']\n",
      "                │   │   │   │       └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ post_attention_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.post_attention_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   └───┬ self_attn:\n",
      "                │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn\n",
      "                │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "                │   │   │       ├───┬ k_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.k_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ o_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.o_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ q_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.q_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       └───┬ v_proj:\n",
      "                │   │   │           ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.v_proj\n",
      "                │   │   │           ├─ children: ['weight']\n",
      "                │   │   │           └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   ├───┬ 4\n",
      "                │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4\n",
      "                │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "                │   │   │   ├───┬ input_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.input_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ mlp\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.mlp\n",
      "                │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "                │   │   │   │   ├───┬ down_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.down_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([960, 2560]) [torch.bfloat16]\n",
      "                │   │   │   │   ├───┬ gate_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.gate_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   │   └───┬ up_proj:\n",
      "                │   │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.up_proj\n",
      "                │   │   │   │       ├─ children: ['weight']\n",
      "                │   │   │   │       └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ post_attention_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.post_attention_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   └───┬ self_attn:\n",
      "                │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn\n",
      "                │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "                │   │   │       ├───┬ k_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.k_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ o_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.o_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ q_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.q_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       └───┬ v_proj:\n",
      "                │   │   │           ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.v_proj\n",
      "                │   │   │           ├─ children: ['weight']\n",
      "                │   │   │           └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   ├───┬ 5\n",
      "                │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5\n",
      "                │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "                │   │   │   ├───┬ input_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.input_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ mlp\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.mlp\n",
      "                │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "                │   │   │   │   ├───┬ down_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.down_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([960, 2560]) [torch.bfloat16]\n",
      "                │   │   │   │   ├───┬ gate_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.gate_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   │   └───┬ up_proj:\n",
      "                │   │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.up_proj\n",
      "                │   │   │   │       ├─ children: ['weight']\n",
      "                │   │   │   │       └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ post_attention_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.post_attention_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   └───┬ self_attn:\n",
      "                │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn\n",
      "                │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "                │   │   │       ├───┬ k_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.k_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ o_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.o_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ q_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.q_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       └───┬ v_proj:\n",
      "                │   │   │           ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.v_proj\n",
      "                │   │   │           ├─ children: ['weight']\n",
      "                │   │   │           └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   ├───┬ 6\n",
      "                │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6\n",
      "                │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "                │   │   │   ├───┬ input_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.input_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ mlp\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.mlp\n",
      "                │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "                │   │   │   │   ├───┬ down_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.down_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([960, 2560]) [torch.bfloat16]\n",
      "                │   │   │   │   ├───┬ gate_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.gate_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   │   └───┬ up_proj:\n",
      "                │   │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.up_proj\n",
      "                │   │   │   │       ├─ children: ['weight']\n",
      "                │   │   │   │       └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ post_attention_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.post_attention_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   └───┬ self_attn:\n",
      "                │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn\n",
      "                │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "                │   │   │       ├───┬ k_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.k_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ o_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.o_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ q_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.q_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       └───┬ v_proj:\n",
      "                │   │   │           ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.v_proj\n",
      "                │   │   │           ├─ children: ['weight']\n",
      "                │   │   │           └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   ├───┬ 7\n",
      "                │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7\n",
      "                │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "                │   │   │   ├───┬ input_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.input_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ mlp\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.mlp\n",
      "                │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "                │   │   │   │   ├───┬ down_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.down_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([960, 2560]) [torch.bfloat16]\n",
      "                │   │   │   │   ├───┬ gate_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.gate_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   │   └───┬ up_proj:\n",
      "                │   │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.up_proj\n",
      "                │   │   │   │       ├─ children: ['weight']\n",
      "                │   │   │   │       └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ post_attention_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.post_attention_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   └───┬ self_attn:\n",
      "                │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn\n",
      "                │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "                │   │   │       ├───┬ k_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.k_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ o_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.o_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ q_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.q_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       └───┬ v_proj:\n",
      "                │   │   │           ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.v_proj\n",
      "                │   │   │           ├─ children: ['weight']\n",
      "                │   │   │           └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   ├───┬ 8\n",
      "                │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8\n",
      "                │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "                │   │   │   ├───┬ input_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.input_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ mlp\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.mlp\n",
      "                │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "                │   │   │   │   ├───┬ down_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.down_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([960, 2560]) [torch.bfloat16]\n",
      "                │   │   │   │   ├───┬ gate_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.gate_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   │   └───┬ up_proj:\n",
      "                │   │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.up_proj\n",
      "                │   │   │   │       ├─ children: ['weight']\n",
      "                │   │   │   │       └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ post_attention_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.post_attention_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   └───┬ self_attn:\n",
      "                │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn\n",
      "                │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "                │   │   │       ├───┬ k_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.k_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ o_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.o_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ q_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.q_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       └───┬ v_proj:\n",
      "                │   │   │           ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.v_proj\n",
      "                │   │   │           ├─ children: ['weight']\n",
      "                │   │   │           └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   ├───┬ 9\n",
      "                │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9\n",
      "                │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "                │   │   │   ├───┬ input_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.input_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ mlp\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.mlp\n",
      "                │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "                │   │   │   │   ├───┬ down_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.down_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([960, 2560]) [torch.bfloat16]\n",
      "                │   │   │   │   ├───┬ gate_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.gate_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   │   └───┬ up_proj:\n",
      "                │   │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.up_proj\n",
      "                │   │   │   │       ├─ children: ['weight']\n",
      "                │   │   │   │       └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ post_attention_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.post_attention_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   └───┬ self_attn:\n",
      "                │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn\n",
      "                │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "                │   │   │       ├───┬ k_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.k_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ o_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.o_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ q_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.q_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       └───┬ v_proj:\n",
      "                │   │   │           ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.v_proj\n",
      "                │   │   │           ├─ children: ['weight']\n",
      "                │   │   │           └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   ├───┬ 10\n",
      "                │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10\n",
      "                │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "                │   │   │   ├───┬ input_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.input_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ mlp\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.mlp\n",
      "                │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "                │   │   │   │   ├───┬ down_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.down_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([960, 2560]) [torch.bfloat16]\n",
      "                │   │   │   │   ├───┬ gate_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.gate_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   │   └───┬ up_proj:\n",
      "                │   │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.up_proj\n",
      "                │   │   │   │       ├─ children: ['weight']\n",
      "                │   │   │   │       └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ post_attention_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.post_attention_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   └───┬ self_attn:\n",
      "                │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn\n",
      "                │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "                │   │   │       ├───┬ k_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.k_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ o_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.o_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ q_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.q_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       └───┬ v_proj:\n",
      "                │   │   │           ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.v_proj\n",
      "                │   │   │           ├─ children: ['weight']\n",
      "                │   │   │           └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   ├───┬ 11\n",
      "                │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11\n",
      "                │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "                │   │   │   ├───┬ input_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.input_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ mlp\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.mlp\n",
      "                │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "                │   │   │   │   ├───┬ down_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.down_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([960, 2560]) [torch.bfloat16]\n",
      "                │   │   │   │   ├───┬ gate_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.gate_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   │   └───┬ up_proj:\n",
      "                │   │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.up_proj\n",
      "                │   │   │   │       ├─ children: ['weight']\n",
      "                │   │   │   │       └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ post_attention_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.post_attention_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   └───┬ self_attn:\n",
      "                │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn\n",
      "                │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "                │   │   │       ├───┬ k_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.k_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ o_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.o_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ q_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.q_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       └───┬ v_proj:\n",
      "                │   │   │           ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.v_proj\n",
      "                │   │   │           ├─ children: ['weight']\n",
      "                │   │   │           └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   ├───┬ 12\n",
      "                │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12\n",
      "                │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "                │   │   │   ├───┬ input_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.input_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ mlp\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.mlp\n",
      "                │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "                │   │   │   │   ├───┬ down_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.down_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([960, 2560]) [torch.bfloat16]\n",
      "                │   │   │   │   ├───┬ gate_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.gate_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   │   └───┬ up_proj:\n",
      "                │   │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.up_proj\n",
      "                │   │   │   │       ├─ children: ['weight']\n",
      "                │   │   │   │       └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ post_attention_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.post_attention_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   └───┬ self_attn:\n",
      "                │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn\n",
      "                │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "                │   │   │       ├───┬ k_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.k_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ o_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.o_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ q_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.q_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       └───┬ v_proj:\n",
      "                │   │   │           ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.v_proj\n",
      "                │   │   │           ├─ children: ['weight']\n",
      "                │   │   │           └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   ├───┬ 13\n",
      "                │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13\n",
      "                │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "                │   │   │   ├───┬ input_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.input_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ mlp\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.mlp\n",
      "                │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "                │   │   │   │   ├───┬ down_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.down_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([960, 2560]) [torch.bfloat16]\n",
      "                │   │   │   │   ├───┬ gate_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.gate_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   │   └───┬ up_proj:\n",
      "                │   │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.up_proj\n",
      "                │   │   │   │       ├─ children: ['weight']\n",
      "                │   │   │   │       └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ post_attention_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.post_attention_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   └───┬ self_attn:\n",
      "                │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn\n",
      "                │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "                │   │   │       ├───┬ k_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.k_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ o_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.o_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ q_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.q_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       └───┬ v_proj:\n",
      "                │   │   │           ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.v_proj\n",
      "                │   │   │           ├─ children: ['weight']\n",
      "                │   │   │           └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   ├───┬ 14\n",
      "                │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14\n",
      "                │   │   |   ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "                │   │   │   ├───┬ input_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.input_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ mlp\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.mlp\n",
      "                │   │   │   |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "                │   │   │   │   ├───┬ down_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.down_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([960, 2560]) [torch.bfloat16]\n",
      "                │   │   │   │   ├───┬ gate_proj\n",
      "                │   │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.gate_proj\n",
      "                │   │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   │   └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   │   └───┬ up_proj:\n",
      "                │   │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.up_proj\n",
      "                │   │   │   │       ├─ children: ['weight']\n",
      "                │   │   │   │       └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │   │   ├───┬ post_attention_layernorm\n",
      "                │   │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.post_attention_layernorm\n",
      "                │   │   │   |   ├─ children: ['weight']\n",
      "                │   │   │   │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │   │   └───┬ self_attn:\n",
      "                │   │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn\n",
      "                │   │   │       ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "                │   │   │       ├───┬ k_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.k_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ o_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.o_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       ├───┬ q_proj\n",
      "                │   │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.q_proj\n",
      "                │   │   │       |   ├─ children: ['weight']\n",
      "                │   │   │       │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │   │       └───┬ v_proj:\n",
      "                │   │   │           ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.v_proj\n",
      "                │   │   │           ├─ children: ['weight']\n",
      "                │   │   │           └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │   └───┬ 15:\n",
      "                │   │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15\n",
      "                │   │       ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "                │   │       ├───┬ input_layernorm\n",
      "                │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.input_layernorm\n",
      "                │   │       |   ├─ children: ['weight']\n",
      "                │   │       │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │       ├───┬ mlp\n",
      "                │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.mlp\n",
      "                │   │       |   ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "                │   │       │   ├───┬ down_proj\n",
      "                │   │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.down_proj\n",
      "                │   │       │   |   ├─ children: ['weight']\n",
      "                │   │       │   │   └──── weight: torch.Size([960, 2560]) [torch.bfloat16]\n",
      "                │   │       │   ├───┬ gate_proj\n",
      "                │   │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.gate_proj\n",
      "                │   │       │   |   ├─ children: ['weight']\n",
      "                │   │       │   │   └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │       │   └───┬ up_proj:\n",
      "                │   │       │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.up_proj\n",
      "                │   │       │       ├─ children: ['weight']\n",
      "                │   │       │       └──── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "                │   │       ├───┬ post_attention_layernorm\n",
      "                │   │       |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.post_attention_layernorm\n",
      "                │   │       |   ├─ children: ['weight']\n",
      "                │   │       │   └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                │   │       └───┬ self_attn:\n",
      "                │   │           ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn\n",
      "                │   │           ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "                │   │           ├───┬ k_proj\n",
      "                │   │           |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.k_proj\n",
      "                │   │           |   ├─ children: ['weight']\n",
      "                │   │           │   └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   │           ├───┬ o_proj\n",
      "                │   │           |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.o_proj\n",
      "                │   │           |   ├─ children: ['weight']\n",
      "                │   │           │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │           ├───┬ q_proj\n",
      "                │   │           |   ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.q_proj\n",
      "                │   │           |   ├─ children: ['weight']\n",
      "                │   │           │   └──── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "                │   │           └───┬ v_proj:\n",
      "                │   │               ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.v_proj\n",
      "                │   │               ├─ children: ['weight']\n",
      "                │   │               └──── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "                │   └───┬ norm:\n",
      "                │       ├─ in: .model.vlm_with_expert.vlm.model.text_model.norm\n",
      "                │       ├─ children: ['weight']\n",
      "                │       └──── weight: torch.Size([960]) [torch.bfloat16]\n",
      "                └───┬ vision_model:\n",
      "                    ├─ in: .model.vlm_with_expert.vlm.model.vision_model\n",
      "                    ├─ children: ['embeddings', 'encoder', 'post_layernorm']\n",
      "                    ├───┬ embeddings\n",
      "                    |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.embeddings\n",
      "                    |   ├─ children: ['patch_embedding', 'position_embedding']\n",
      "                    │   ├───┬ patch_embedding\n",
      "                    │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.embeddings.patch_embedding\n",
      "                    │   |   ├─ children: ['bias', 'weight']\n",
      "                    │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │   │   └──── weight: torch.Size([768, 3, 16, 16]) [torch.bfloat16]\n",
      "                    │   └───┬ position_embedding:\n",
      "                    │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.embeddings.position_embedding\n",
      "                    │       ├─ children: ['weight']\n",
      "                    │       └──── weight: torch.Size([1024, 768]) [torch.bfloat16]\n",
      "                    ├───┬ encoder\n",
      "                    |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder\n",
      "                    |   ├─ children: ['layers']\n",
      "                    │   └───┬ layers:\n",
      "                    │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers\n",
      "                    │       ├─ children: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']\n",
      "                    │       ├───┬ 0\n",
      "                    │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0\n",
      "                    │       |   ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']\n",
      "                    │       │   ├───┬ layer_norm1\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm1\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ layer_norm2\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm2\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ mlp\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp\n",
      "                    │       │   |   ├─ children: ['fc1', 'fc2']\n",
      "                    │       │   │   ├───┬ fc1\n",
      "                    │       │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc1\n",
      "                    │       │   │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   │   ├──── bias: torch.Size([3072]) [torch.bfloat16]\n",
      "                    │       │   │   │   └──── weight: torch.Size([3072, 768]) [torch.bfloat16]\n",
      "                    │       │   │   └───┬ fc2:\n",
      "                    │       │   │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc2\n",
      "                    │       │   │       ├─ children: ['bias', 'weight']\n",
      "                    │       │   │       ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │       └──── weight: torch.Size([768, 3072]) [torch.bfloat16]\n",
      "                    │       │   └───┬ self_attn:\n",
      "                    │       │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn\n",
      "                    │       │       ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']\n",
      "                    │       │       ├───┬ k_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.k_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ out_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.out_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ q_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.q_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       └───┬ v_proj:\n",
      "                    │       │           ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.v_proj\n",
      "                    │       │           ├─ children: ['bias', 'weight']\n",
      "                    │       │           ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │           └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       ├───┬ 1\n",
      "                    │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1\n",
      "                    │       |   ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']\n",
      "                    │       │   ├───┬ layer_norm1\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm1\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ layer_norm2\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm2\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ mlp\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp\n",
      "                    │       │   |   ├─ children: ['fc1', 'fc2']\n",
      "                    │       │   │   ├───┬ fc1\n",
      "                    │       │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc1\n",
      "                    │       │   │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   │   ├──── bias: torch.Size([3072]) [torch.bfloat16]\n",
      "                    │       │   │   │   └──── weight: torch.Size([3072, 768]) [torch.bfloat16]\n",
      "                    │       │   │   └───┬ fc2:\n",
      "                    │       │   │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc2\n",
      "                    │       │   │       ├─ children: ['bias', 'weight']\n",
      "                    │       │   │       ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │       └──── weight: torch.Size([768, 3072]) [torch.bfloat16]\n",
      "                    │       │   └───┬ self_attn:\n",
      "                    │       │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn\n",
      "                    │       │       ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']\n",
      "                    │       │       ├───┬ k_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.k_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ out_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.out_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ q_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.q_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       └───┬ v_proj:\n",
      "                    │       │           ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.v_proj\n",
      "                    │       │           ├─ children: ['bias', 'weight']\n",
      "                    │       │           ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │           └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       ├───┬ 2\n",
      "                    │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2\n",
      "                    │       |   ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']\n",
      "                    │       │   ├───┬ layer_norm1\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm1\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ layer_norm2\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm2\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ mlp\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp\n",
      "                    │       │   |   ├─ children: ['fc1', 'fc2']\n",
      "                    │       │   │   ├───┬ fc1\n",
      "                    │       │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc1\n",
      "                    │       │   │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   │   ├──── bias: torch.Size([3072]) [torch.bfloat16]\n",
      "                    │       │   │   │   └──── weight: torch.Size([3072, 768]) [torch.bfloat16]\n",
      "                    │       │   │   └───┬ fc2:\n",
      "                    │       │   │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc2\n",
      "                    │       │   │       ├─ children: ['bias', 'weight']\n",
      "                    │       │   │       ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │       └──── weight: torch.Size([768, 3072]) [torch.bfloat16]\n",
      "                    │       │   └───┬ self_attn:\n",
      "                    │       │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn\n",
      "                    │       │       ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']\n",
      "                    │       │       ├───┬ k_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.k_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ out_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.out_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ q_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.q_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       └───┬ v_proj:\n",
      "                    │       │           ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.v_proj\n",
      "                    │       │           ├─ children: ['bias', 'weight']\n",
      "                    │       │           ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │           └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       ├───┬ 3\n",
      "                    │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3\n",
      "                    │       |   ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']\n",
      "                    │       │   ├───┬ layer_norm1\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm1\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ layer_norm2\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm2\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ mlp\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp\n",
      "                    │       │   |   ├─ children: ['fc1', 'fc2']\n",
      "                    │       │   │   ├───┬ fc1\n",
      "                    │       │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc1\n",
      "                    │       │   │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   │   ├──── bias: torch.Size([3072]) [torch.bfloat16]\n",
      "                    │       │   │   │   └──── weight: torch.Size([3072, 768]) [torch.bfloat16]\n",
      "                    │       │   │   └───┬ fc2:\n",
      "                    │       │   │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc2\n",
      "                    │       │   │       ├─ children: ['bias', 'weight']\n",
      "                    │       │   │       ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │       └──── weight: torch.Size([768, 3072]) [torch.bfloat16]\n",
      "                    │       │   └───┬ self_attn:\n",
      "                    │       │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn\n",
      "                    │       │       ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']\n",
      "                    │       │       ├───┬ k_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.k_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ out_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.out_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ q_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.q_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       └───┬ v_proj:\n",
      "                    │       │           ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.v_proj\n",
      "                    │       │           ├─ children: ['bias', 'weight']\n",
      "                    │       │           ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │           └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       ├───┬ 4\n",
      "                    │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4\n",
      "                    │       |   ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']\n",
      "                    │       │   ├───┬ layer_norm1\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm1\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ layer_norm2\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm2\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ mlp\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp\n",
      "                    │       │   |   ├─ children: ['fc1', 'fc2']\n",
      "                    │       │   │   ├───┬ fc1\n",
      "                    │       │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc1\n",
      "                    │       │   │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   │   ├──── bias: torch.Size([3072]) [torch.bfloat16]\n",
      "                    │       │   │   │   └──── weight: torch.Size([3072, 768]) [torch.bfloat16]\n",
      "                    │       │   │   └───┬ fc2:\n",
      "                    │       │   │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc2\n",
      "                    │       │   │       ├─ children: ['bias', 'weight']\n",
      "                    │       │   │       ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │       └──── weight: torch.Size([768, 3072]) [torch.bfloat16]\n",
      "                    │       │   └───┬ self_attn:\n",
      "                    │       │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn\n",
      "                    │       │       ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']\n",
      "                    │       │       ├───┬ k_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.k_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ out_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.out_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ q_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.q_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       └───┬ v_proj:\n",
      "                    │       │           ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.v_proj\n",
      "                    │       │           ├─ children: ['bias', 'weight']\n",
      "                    │       │           ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │           └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       ├───┬ 5\n",
      "                    │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5\n",
      "                    │       |   ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']\n",
      "                    │       │   ├───┬ layer_norm1\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm1\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ layer_norm2\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm2\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ mlp\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp\n",
      "                    │       │   |   ├─ children: ['fc1', 'fc2']\n",
      "                    │       │   │   ├───┬ fc1\n",
      "                    │       │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc1\n",
      "                    │       │   │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   │   ├──── bias: torch.Size([3072]) [torch.bfloat16]\n",
      "                    │       │   │   │   └──── weight: torch.Size([3072, 768]) [torch.bfloat16]\n",
      "                    │       │   │   └───┬ fc2:\n",
      "                    │       │   │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc2\n",
      "                    │       │   │       ├─ children: ['bias', 'weight']\n",
      "                    │       │   │       ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │       └──── weight: torch.Size([768, 3072]) [torch.bfloat16]\n",
      "                    │       │   └───┬ self_attn:\n",
      "                    │       │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn\n",
      "                    │       │       ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']\n",
      "                    │       │       ├───┬ k_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.k_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ out_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.out_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ q_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.q_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       └───┬ v_proj:\n",
      "                    │       │           ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.v_proj\n",
      "                    │       │           ├─ children: ['bias', 'weight']\n",
      "                    │       │           ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │           └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       ├───┬ 6\n",
      "                    │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6\n",
      "                    │       |   ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']\n",
      "                    │       │   ├───┬ layer_norm1\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm1\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ layer_norm2\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm2\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ mlp\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp\n",
      "                    │       │   |   ├─ children: ['fc1', 'fc2']\n",
      "                    │       │   │   ├───┬ fc1\n",
      "                    │       │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc1\n",
      "                    │       │   │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   │   ├──── bias: torch.Size([3072]) [torch.bfloat16]\n",
      "                    │       │   │   │   └──── weight: torch.Size([3072, 768]) [torch.bfloat16]\n",
      "                    │       │   │   └───┬ fc2:\n",
      "                    │       │   │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc2\n",
      "                    │       │   │       ├─ children: ['bias', 'weight']\n",
      "                    │       │   │       ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │       └──── weight: torch.Size([768, 3072]) [torch.bfloat16]\n",
      "                    │       │   └───┬ self_attn:\n",
      "                    │       │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn\n",
      "                    │       │       ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']\n",
      "                    │       │       ├───┬ k_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.k_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ out_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.out_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ q_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.q_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       └───┬ v_proj:\n",
      "                    │       │           ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.v_proj\n",
      "                    │       │           ├─ children: ['bias', 'weight']\n",
      "                    │       │           ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │           └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       ├───┬ 7\n",
      "                    │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7\n",
      "                    │       |   ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']\n",
      "                    │       │   ├───┬ layer_norm1\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm1\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ layer_norm2\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm2\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ mlp\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp\n",
      "                    │       │   |   ├─ children: ['fc1', 'fc2']\n",
      "                    │       │   │   ├───┬ fc1\n",
      "                    │       │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc1\n",
      "                    │       │   │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   │   ├──── bias: torch.Size([3072]) [torch.bfloat16]\n",
      "                    │       │   │   │   └──── weight: torch.Size([3072, 768]) [torch.bfloat16]\n",
      "                    │       │   │   └───┬ fc2:\n",
      "                    │       │   │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc2\n",
      "                    │       │   │       ├─ children: ['bias', 'weight']\n",
      "                    │       │   │       ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │       └──── weight: torch.Size([768, 3072]) [torch.bfloat16]\n",
      "                    │       │   └───┬ self_attn:\n",
      "                    │       │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn\n",
      "                    │       │       ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']\n",
      "                    │       │       ├───┬ k_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.k_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ out_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.out_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ q_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.q_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       └───┬ v_proj:\n",
      "                    │       │           ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.v_proj\n",
      "                    │       │           ├─ children: ['bias', 'weight']\n",
      "                    │       │           ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │           └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       ├───┬ 8\n",
      "                    │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8\n",
      "                    │       |   ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']\n",
      "                    │       │   ├───┬ layer_norm1\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm1\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ layer_norm2\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm2\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ mlp\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp\n",
      "                    │       │   |   ├─ children: ['fc1', 'fc2']\n",
      "                    │       │   │   ├───┬ fc1\n",
      "                    │       │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc1\n",
      "                    │       │   │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   │   ├──── bias: torch.Size([3072]) [torch.bfloat16]\n",
      "                    │       │   │   │   └──── weight: torch.Size([3072, 768]) [torch.bfloat16]\n",
      "                    │       │   │   └───┬ fc2:\n",
      "                    │       │   │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc2\n",
      "                    │       │   │       ├─ children: ['bias', 'weight']\n",
      "                    │       │   │       ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │       └──── weight: torch.Size([768, 3072]) [torch.bfloat16]\n",
      "                    │       │   └───┬ self_attn:\n",
      "                    │       │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn\n",
      "                    │       │       ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']\n",
      "                    │       │       ├───┬ k_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.k_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ out_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.out_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ q_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.q_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       └───┬ v_proj:\n",
      "                    │       │           ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.v_proj\n",
      "                    │       │           ├─ children: ['bias', 'weight']\n",
      "                    │       │           ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │           └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       ├───┬ 9\n",
      "                    │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9\n",
      "                    │       |   ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']\n",
      "                    │       │   ├───┬ layer_norm1\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm1\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ layer_norm2\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm2\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ mlp\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp\n",
      "                    │       │   |   ├─ children: ['fc1', 'fc2']\n",
      "                    │       │   │   ├───┬ fc1\n",
      "                    │       │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc1\n",
      "                    │       │   │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   │   ├──── bias: torch.Size([3072]) [torch.bfloat16]\n",
      "                    │       │   │   │   └──── weight: torch.Size([3072, 768]) [torch.bfloat16]\n",
      "                    │       │   │   └───┬ fc2:\n",
      "                    │       │   │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc2\n",
      "                    │       │   │       ├─ children: ['bias', 'weight']\n",
      "                    │       │   │       ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │       └──── weight: torch.Size([768, 3072]) [torch.bfloat16]\n",
      "                    │       │   └───┬ self_attn:\n",
      "                    │       │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn\n",
      "                    │       │       ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']\n",
      "                    │       │       ├───┬ k_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.k_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ out_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.out_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ q_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.q_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       └───┬ v_proj:\n",
      "                    │       │           ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.v_proj\n",
      "                    │       │           ├─ children: ['bias', 'weight']\n",
      "                    │       │           ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │           └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       ├───┬ 10\n",
      "                    │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10\n",
      "                    │       |   ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']\n",
      "                    │       │   ├───┬ layer_norm1\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm1\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ layer_norm2\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm2\n",
      "                    │       │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   ├───┬ mlp\n",
      "                    │       │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp\n",
      "                    │       │   |   ├─ children: ['fc1', 'fc2']\n",
      "                    │       │   │   ├───┬ fc1\n",
      "                    │       │   │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc1\n",
      "                    │       │   │   |   ├─ children: ['bias', 'weight']\n",
      "                    │       │   │   │   ├──── bias: torch.Size([3072]) [torch.bfloat16]\n",
      "                    │       │   │   │   └──── weight: torch.Size([3072, 768]) [torch.bfloat16]\n",
      "                    │       │   │   └───┬ fc2:\n",
      "                    │       │   │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc2\n",
      "                    │       │   │       ├─ children: ['bias', 'weight']\n",
      "                    │       │   │       ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │   │       └──── weight: torch.Size([768, 3072]) [torch.bfloat16]\n",
      "                    │       │   └───┬ self_attn:\n",
      "                    │       │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn\n",
      "                    │       │       ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']\n",
      "                    │       │       ├───┬ k_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.k_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ out_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.out_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       ├───┬ q_proj\n",
      "                    │       │       |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.q_proj\n",
      "                    │       │       |   ├─ children: ['bias', 'weight']\n",
      "                    │       │       │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │       │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       │       └───┬ v_proj:\n",
      "                    │       │           ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.v_proj\n",
      "                    │       │           ├─ children: ['bias', 'weight']\n",
      "                    │       │           ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │       │           └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │       └───┬ 11:\n",
      "                    │           ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11\n",
      "                    │           ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']\n",
      "                    │           ├───┬ layer_norm1\n",
      "                    │           |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm1\n",
      "                    │           |   ├─ children: ['bias', 'weight']\n",
      "                    │           │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │           │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │           ├───┬ layer_norm2\n",
      "                    │           |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm2\n",
      "                    │           |   ├─ children: ['bias', 'weight']\n",
      "                    │           │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │           │   └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "                    │           ├───┬ mlp\n",
      "                    │           |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp\n",
      "                    │           |   ├─ children: ['fc1', 'fc2']\n",
      "                    │           │   ├───┬ fc1\n",
      "                    │           │   |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc1\n",
      "                    │           │   |   ├─ children: ['bias', 'weight']\n",
      "                    │           │   │   ├──── bias: torch.Size([3072]) [torch.bfloat16]\n",
      "                    │           │   │   └──── weight: torch.Size([3072, 768]) [torch.bfloat16]\n",
      "                    │           │   └───┬ fc2:\n",
      "                    │           │       ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc2\n",
      "                    │           │       ├─ children: ['bias', 'weight']\n",
      "                    │           │       ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │           │       └──── weight: torch.Size([768, 3072]) [torch.bfloat16]\n",
      "                    │           └───┬ self_attn:\n",
      "                    │               ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn\n",
      "                    │               ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']\n",
      "                    │               ├───┬ k_proj\n",
      "                    │               |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.k_proj\n",
      "                    │               |   ├─ children: ['bias', 'weight']\n",
      "                    │               │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │               │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │               ├───┬ out_proj\n",
      "                    │               |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.out_proj\n",
      "                    │               |   ├─ children: ['bias', 'weight']\n",
      "                    │               │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │               │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │               ├───┬ q_proj\n",
      "                    │               |   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.q_proj\n",
      "                    │               |   ├─ children: ['bias', 'weight']\n",
      "                    │               │   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │               │   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    │               └───┬ v_proj:\n",
      "                    │                   ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.v_proj\n",
      "                    │                   ├─ children: ['bias', 'weight']\n",
      "                    │                   ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                    │                   └──── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "                    └───┬ post_layernorm:\n",
      "                        ├─ in: .model.vlm_with_expert.vlm.model.vision_model.post_layernorm\n",
      "                        ├─ children: ['bias', 'weight']\n",
      "                        ├──── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                        └──── weight: torch.Size([768]) [torch.bfloat16]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_tree = make_dictionary_tree(state)\n",
    "\n",
    "tree_string = dictionary_tree_as_string(\n",
    "    state_tree\n",
    ")\n",
    "\n",
    "with open(f\"{out_dir}/model_shape.txt\", \"w\") as f:\n",
    "    f.write(tree_string)\n",
    "\n",
    "print(tree_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3250c2",
   "metadata": {},
   "source": [
    "4. Hone in on an attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec02e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_selected_layer(\n",
    "    state_tree,\n",
    "    layer\n",
    "):\n",
    "    keys = layer.split('.')\n",
    "    selected_layer = state_tree\n",
    "    for key in keys:\n",
    "        if key not in selected_layer:\n",
    "            print (f\"Layer {layer} not found in state tree. (stopped at key '{key}')\")\n",
    "            print (\"Available keys at this level:\", list(selected_layer.keys()))\n",
    "            return \n",
    "        \n",
    "        selected_layer = selected_layer[key]\n",
    "\n",
    "    \n",
    "    selected_layer_string = dictionary_tree_as_string(\n",
    "        selected_layer,\n",
    "        key_prefix=layer\n",
    "    )\n",
    "\n",
    "    with open(f\"{out_dir}/selected_layer_{layer.replace('.', '_')}.txt\", \"w\") as f:\n",
    "        f.write(selected_layer_string)\n",
    "\n",
    "layers_to_save = [\n",
    "    'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6',\n",
    "    'model.vlm_with_expert.vlm.model.text_model.layers.3',\n",
    "    'model.vlm_with_expert.lm_expert.layers.2',\n",
    "]\n",
    "\n",
    "for layer in layers_to_save:\n",
    "    save_selected_layer(\n",
    "        state_tree,\n",
    "        layer=layer\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caafdbbb",
   "metadata": {},
   "source": [
    "5. Get the model weights for a given layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121cb58",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41cac220",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shapes = {}\n",
    "\n",
    "for state_name, tensor in state.items():\n",
    "    shapes = tensor.shape\n",
    "\n",
    "    for shape in shapes:\n",
    "        if shape not in state_shapes:\n",
    "            state_shapes[shape] = set()\n",
    "        state_shapes[shape].add(f\"{state_name}: {tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ea8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smolVLA-conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
