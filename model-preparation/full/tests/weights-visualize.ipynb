{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10941ba",
   "metadata": {},
   "source": [
    "# Setup\n",
    " - Point to your weights\n",
    " - Pick a tiny input shape for vectors\n",
    " - All outputs go to ./smolvla_test_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5325ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os, json, math, re, pathlib, itertools\n",
    "from typing import Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# <<< EDIT IF NEEDED >>>\n",
    "model_weights = \"../weights/downloads/model.safetensors\"  # <= your path\n",
    "out_dir = \"./smolvla_test_vectors\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff10143",
   "metadata": {},
   "source": [
    "1) Load safetensors and index keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f7b8335",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = load_safetensors(model_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9165f18f",
   "metadata": {},
   "source": [
    "2. Helper functions for printing out and understanding model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fa817b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import true\n",
    "\n",
    "\n",
    "def make_dictionary_tree(dict, split='.'):\n",
    "    tree = {}\n",
    "\n",
    "    for key, value in dict.items():\n",
    "        parts = key.split(split)\n",
    "        tree_key = parts[0]\n",
    "        if len(parts) == 1:\n",
    "            tree[tree_key] = value\n",
    "            continue\n",
    "        else:\n",
    "            subtree_key = split.join(parts[1:])\n",
    "            \n",
    "            if tree_key not in tree:\n",
    "                tree[tree_key] = { subtree_key: value }\n",
    "            else:\n",
    "                tree[tree_key][subtree_key] = value\n",
    "\n",
    "    \n",
    "    # if all items are numbers, sort them by numeric order\n",
    "    if all(re.match(r'^\\d+$', str(k)) for k, v in tree.items()):\n",
    "        tree = { k: v for k, v in sorted(tree.items(), key=lambda x: int(x[0])) }\n",
    "\n",
    "    for key in tree.keys():\n",
    "        if isinstance(tree[key], Dict):\n",
    "            tree[key] = make_dictionary_tree(tree[key], split=split)    \n",
    "\n",
    "    return tree\n",
    "\n",
    "def dictionary_tree_as_string(tree, prefix='', key_prefix='', simplify_numbered_keys=False):\n",
    "    items = list(tree.items())\n",
    "    result = \"\"\n",
    "    \n",
    "    SPACE_LENGTH = 2\n",
    "    \n",
    "    if simplify_numbered_keys:\n",
    "        # check if all keys are numbers\n",
    "        if all(re.match(r'^\\d+$', str(k)) for k, v in items):\n",
    "            items = [(int(k), v) for k, v in items]\n",
    "            new_name = f\"[{items[0][0]}..{items[-1][0]}]\"\n",
    "            items = [(new_name, items[0][1])]\n",
    "\n",
    "    for key, value in items[:-1]:\n",
    "        if isinstance(value, dict):\n",
    "            result += f\"{prefix}├{'─' * SPACE_LENGTH}┬ {key}\\n\"\n",
    "            if key_prefix:\n",
    "                result += f\"{prefix}|{' ' * SPACE_LENGTH}├{'─' * (SPACE_LENGTH // 2)} in: {key_prefix}.{key}\\n\"\n",
    "            if isinstance(value, dict):\n",
    "                result += f\"{prefix}|{' ' * SPACE_LENGTH}├{'─' * (SPACE_LENGTH // 2)} children: {list(value.keys())}\\n\"\n",
    "            result += dictionary_tree_as_string(\n",
    "                value, \n",
    "                f\"{prefix}│{' ' * SPACE_LENGTH}\",\n",
    "                f\"{key_prefix}.{key}\",\n",
    "                simplify_numbered_keys=simplify_numbered_keys\n",
    "            )\n",
    "        else:\n",
    "            result += f\"{prefix}├{'─' * SPACE_LENGTH} {key}: {value.shape} [{value.dtype}]\\n\"\n",
    "\n",
    "    last_key, last_value = items[-1]\n",
    "    if isinstance(last_value, dict):\n",
    "        if True:\n",
    "            result += f\"{prefix}└{'─' * SPACE_LENGTH}┬ {last_key}:\\n\"\n",
    "        if key_prefix:\n",
    "            result += f\"{prefix} {' ' * SPACE_LENGTH}├{'─' * (SPACE_LENGTH // 2)} in: {key_prefix}.{last_key}\\n\"\n",
    "        if isinstance(last_value, dict):\n",
    "            result += f\"{prefix} {' ' * SPACE_LENGTH}├{'─' * (SPACE_LENGTH // 2)} children: {list(last_value.keys())}\\n\"\n",
    "        result += dictionary_tree_as_string(\n",
    "            last_value, \n",
    "            f\"{prefix} {' ' * SPACE_LENGTH}\",\n",
    "            f\"{key_prefix}.{last_key}\",\n",
    "            simplify_numbered_keys=simplify_numbered_keys\n",
    "        )\n",
    "    else:\n",
    "        result += f\"{prefix}└{'─' * SPACE_LENGTH} {last_key}: {last_value.shape} [{last_value.dtype}]\\n\"\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c23ef8f",
   "metadata": {},
   "source": [
    "3. prints out the actual weights as a nice dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1da5b001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "└──┬ model:\n",
      "   ├─ children: ['action_in_proj', 'action_out_proj', 'action_time_mlp_in', 'action_time_mlp_out', 'state_proj', 'vlm_with_expert']\n",
      "   ├──┬ action_in_proj\n",
      "   |  ├─ in: .model.action_in_proj\n",
      "   |  ├─ children: ['bias', 'weight']\n",
      "   │  ├── bias: torch.Size([720]) [torch.float32]\n",
      "   │  └── weight: torch.Size([720, 32]) [torch.float32]\n",
      "   ├──┬ action_out_proj\n",
      "   |  ├─ in: .model.action_out_proj\n",
      "   |  ├─ children: ['bias', 'weight']\n",
      "   │  ├── bias: torch.Size([32]) [torch.float32]\n",
      "   │  └── weight: torch.Size([32, 720]) [torch.float32]\n",
      "   ├──┬ action_time_mlp_in\n",
      "   |  ├─ in: .model.action_time_mlp_in\n",
      "   |  ├─ children: ['bias', 'weight']\n",
      "   │  ├── bias: torch.Size([720]) [torch.float32]\n",
      "   │  └── weight: torch.Size([720, 1440]) [torch.float32]\n",
      "   ├──┬ action_time_mlp_out\n",
      "   |  ├─ in: .model.action_time_mlp_out\n",
      "   |  ├─ children: ['bias', 'weight']\n",
      "   │  ├── bias: torch.Size([720]) [torch.float32]\n",
      "   │  └── weight: torch.Size([720, 720]) [torch.float32]\n",
      "   ├──┬ state_proj\n",
      "   |  ├─ in: .model.state_proj\n",
      "   |  ├─ children: ['bias', 'weight']\n",
      "   │  ├── bias: torch.Size([960]) [torch.float32]\n",
      "   │  └── weight: torch.Size([960, 32]) [torch.float32]\n",
      "   └──┬ vlm_with_expert:\n",
      "      ├─ in: .model.vlm_with_expert\n",
      "      ├─ children: ['lm_expert', 'vlm']\n",
      "      ├──┬ lm_expert\n",
      "      |  ├─ in: .model.vlm_with_expert.lm_expert\n",
      "      |  ├─ children: ['layers', 'norm']\n",
      "      │  ├──┬ layers\n",
      "      │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers\n",
      "      │  |  ├─ children: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15']\n",
      "      │  │  └──┬ [0..15]:\n",
      "      │  │     ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15]\n",
      "      │  │     ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "      │  │     ├──┬ input_layernorm\n",
      "      │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].input_layernorm\n",
      "      │  │     |  ├─ children: ['weight']\n",
      "      │  │     │  └── weight: torch.Size([720]) [torch.bfloat16]\n",
      "      │  │     ├──┬ mlp\n",
      "      │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].mlp\n",
      "      │  │     |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "      │  │     │  ├──┬ down_proj\n",
      "      │  │     │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].mlp.down_proj\n",
      "      │  │     │  |  ├─ children: ['weight']\n",
      "      │  │     │  │  └── weight: torch.Size([720, 2048]) [torch.bfloat16]\n",
      "      │  │     │  ├──┬ gate_proj\n",
      "      │  │     │  |  ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].mlp.gate_proj\n",
      "      │  │     │  |  ├─ children: ['weight']\n",
      "      │  │     │  │  └── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "      │  │     │  └──┬ up_proj:\n",
      "      │  │     │     ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].mlp.up_proj\n",
      "      │  │     │     ├─ children: ['weight']\n",
      "      │  │     │     └── weight: torch.Size([2048, 720]) [torch.bfloat16]\n",
      "      │  │     ├──┬ post_attention_layernorm\n",
      "      │  │     |  ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].post_attention_layernorm\n",
      "      │  │     |  ├─ children: ['weight']\n",
      "      │  │     │  └── weight: torch.Size([720]) [torch.bfloat16]\n",
      "      │  │     └──┬ self_attn:\n",
      "      │  │        ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].self_attn\n",
      "      │  │        ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "      │  │        ├──┬ k_proj\n",
      "      │  │        |  ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].self_attn.k_proj\n",
      "      │  │        |  ├─ children: ['weight']\n",
      "      │  │        │  └── weight: torch.Size([320, 720]) [torch.bfloat16]\n",
      "      │  │        ├──┬ o_proj\n",
      "      │  │        |  ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].self_attn.o_proj\n",
      "      │  │        |  ├─ children: ['weight']\n",
      "      │  │        │  └── weight: torch.Size([720, 960]) [torch.bfloat16]\n",
      "      │  │        ├──┬ q_proj\n",
      "      │  │        |  ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].self_attn.q_proj\n",
      "      │  │        |  ├─ children: ['weight']\n",
      "      │  │        │  └── weight: torch.Size([960, 720]) [torch.bfloat16]\n",
      "      │  │        └──┬ v_proj:\n",
      "      │  │           ├─ in: .model.vlm_with_expert.lm_expert.layers.[0..15].self_attn.v_proj\n",
      "      │  │           ├─ children: ['weight']\n",
      "      │  │           └── weight: torch.Size([320, 720]) [torch.bfloat16]\n",
      "      │  └──┬ norm:\n",
      "      │     ├─ in: .model.vlm_with_expert.lm_expert.norm\n",
      "      │     ├─ children: ['weight']\n",
      "      │     └── weight: torch.Size([720]) [torch.bfloat16]\n",
      "      └──┬ vlm:\n",
      "         ├─ in: .model.vlm_with_expert.vlm\n",
      "         ├─ children: ['lm_head', 'model']\n",
      "         ├──┬ lm_head\n",
      "         |  ├─ in: .model.vlm_with_expert.vlm.lm_head\n",
      "         |  ├─ children: ['weight']\n",
      "         │  └── weight: torch.Size([49280, 960]) [torch.bfloat16]\n",
      "         └──┬ model:\n",
      "            ├─ in: .model.vlm_with_expert.vlm.model\n",
      "            ├─ children: ['connector', 'text_model', 'vision_model']\n",
      "            ├──┬ connector\n",
      "            |  ├─ in: .model.vlm_with_expert.vlm.model.connector\n",
      "            |  ├─ children: ['modality_projection']\n",
      "            │  └──┬ modality_projection:\n",
      "            │     ├─ in: .model.vlm_with_expert.vlm.model.connector.modality_projection\n",
      "            │     ├─ children: ['proj']\n",
      "            │     └──┬ proj:\n",
      "            │        ├─ in: .model.vlm_with_expert.vlm.model.connector.modality_projection.proj\n",
      "            │        ├─ children: ['weight']\n",
      "            │        └── weight: torch.Size([960, 12288]) [torch.bfloat16]\n",
      "            ├──┬ text_model\n",
      "            |  ├─ in: .model.vlm_with_expert.vlm.model.text_model\n",
      "            |  ├─ children: ['embed_tokens', 'layers', 'norm']\n",
      "            │  ├──┬ embed_tokens\n",
      "            │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.embed_tokens\n",
      "            │  |  ├─ children: ['weight']\n",
      "            │  │  └── weight: torch.Size([49280, 960]) [torch.bfloat16]\n",
      "            │  ├──┬ layers\n",
      "            │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers\n",
      "            │  |  ├─ children: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15']\n",
      "            │  │  └──┬ [0..15]:\n",
      "            │  │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15]\n",
      "            │  │     ├─ children: ['input_layernorm', 'mlp', 'post_attention_layernorm', 'self_attn']\n",
      "            │  │     ├──┬ input_layernorm\n",
      "            │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].input_layernorm\n",
      "            │  │     |  ├─ children: ['weight']\n",
      "            │  │     │  └── weight: torch.Size([960]) [torch.bfloat16]\n",
      "            │  │     ├──┬ mlp\n",
      "            │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].mlp\n",
      "            │  │     |  ├─ children: ['down_proj', 'gate_proj', 'up_proj']\n",
      "            │  │     │  ├──┬ down_proj\n",
      "            │  │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].mlp.down_proj\n",
      "            │  │     │  |  ├─ children: ['weight']\n",
      "            │  │     │  │  └── weight: torch.Size([960, 2560]) [torch.bfloat16]\n",
      "            │  │     │  ├──┬ gate_proj\n",
      "            │  │     │  |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].mlp.gate_proj\n",
      "            │  │     │  |  ├─ children: ['weight']\n",
      "            │  │     │  │  └── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "            │  │     │  └──┬ up_proj:\n",
      "            │  │     │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].mlp.up_proj\n",
      "            │  │     │     ├─ children: ['weight']\n",
      "            │  │     │     └── weight: torch.Size([2560, 960]) [torch.bfloat16]\n",
      "            │  │     ├──┬ post_attention_layernorm\n",
      "            │  │     |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].post_attention_layernorm\n",
      "            │  │     |  ├─ children: ['weight']\n",
      "            │  │     │  └── weight: torch.Size([960]) [torch.bfloat16]\n",
      "            │  │     └──┬ self_attn:\n",
      "            │  │        ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].self_attn\n",
      "            │  │        ├─ children: ['k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "            │  │        ├──┬ k_proj\n",
      "            │  │        |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].self_attn.k_proj\n",
      "            │  │        |  ├─ children: ['weight']\n",
      "            │  │        │  └── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "            │  │        ├──┬ o_proj\n",
      "            │  │        |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].self_attn.o_proj\n",
      "            │  │        |  ├─ children: ['weight']\n",
      "            │  │        │  └── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "            │  │        ├──┬ q_proj\n",
      "            │  │        |  ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].self_attn.q_proj\n",
      "            │  │        |  ├─ children: ['weight']\n",
      "            │  │        │  └── weight: torch.Size([960, 960]) [torch.bfloat16]\n",
      "            │  │        └──┬ v_proj:\n",
      "            │  │           ├─ in: .model.vlm_with_expert.vlm.model.text_model.layers.[0..15].self_attn.v_proj\n",
      "            │  │           ├─ children: ['weight']\n",
      "            │  │           └── weight: torch.Size([320, 960]) [torch.bfloat16]\n",
      "            │  └──┬ norm:\n",
      "            │     ├─ in: .model.vlm_with_expert.vlm.model.text_model.norm\n",
      "            │     ├─ children: ['weight']\n",
      "            │     └── weight: torch.Size([960]) [torch.bfloat16]\n",
      "            └──┬ vision_model:\n",
      "               ├─ in: .model.vlm_with_expert.vlm.model.vision_model\n",
      "               ├─ children: ['embeddings', 'encoder', 'post_layernorm']\n",
      "               ├──┬ embeddings\n",
      "               |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.embeddings\n",
      "               |  ├─ children: ['patch_embedding', 'position_embedding']\n",
      "               │  ├──┬ patch_embedding\n",
      "               │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.embeddings.patch_embedding\n",
      "               │  |  ├─ children: ['bias', 'weight']\n",
      "               │  │  ├── bias: torch.Size([768]) [torch.bfloat16]\n",
      "               │  │  └── weight: torch.Size([768, 3, 16, 16]) [torch.bfloat16]\n",
      "               │  └──┬ position_embedding:\n",
      "               │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.embeddings.position_embedding\n",
      "               │     ├─ children: ['weight']\n",
      "               │     └── weight: torch.Size([1024, 768]) [torch.bfloat16]\n",
      "               ├──┬ encoder\n",
      "               |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder\n",
      "               |  ├─ children: ['layers']\n",
      "               │  └──┬ layers:\n",
      "               │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers\n",
      "               │     ├─ children: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']\n",
      "               │     └──┬ [0..11]:\n",
      "               │        ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11]\n",
      "               │        ├─ children: ['layer_norm1', 'layer_norm2', 'mlp', 'self_attn']\n",
      "               │        ├──┬ layer_norm1\n",
      "               │        |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].layer_norm1\n",
      "               │        |  ├─ children: ['bias', 'weight']\n",
      "               │        │  ├── bias: torch.Size([768]) [torch.bfloat16]\n",
      "               │        │  └── weight: torch.Size([768]) [torch.bfloat16]\n",
      "               │        ├──┬ layer_norm2\n",
      "               │        |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].layer_norm2\n",
      "               │        |  ├─ children: ['bias', 'weight']\n",
      "               │        │  ├── bias: torch.Size([768]) [torch.bfloat16]\n",
      "               │        │  └── weight: torch.Size([768]) [torch.bfloat16]\n",
      "               │        ├──┬ mlp\n",
      "               │        |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].mlp\n",
      "               │        |  ├─ children: ['fc1', 'fc2']\n",
      "               │        │  ├──┬ fc1\n",
      "               │        │  |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].mlp.fc1\n",
      "               │        │  |  ├─ children: ['bias', 'weight']\n",
      "               │        │  │  ├── bias: torch.Size([3072]) [torch.bfloat16]\n",
      "               │        │  │  └── weight: torch.Size([3072, 768]) [torch.bfloat16]\n",
      "               │        │  └──┬ fc2:\n",
      "               │        │     ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].mlp.fc2\n",
      "               │        │     ├─ children: ['bias', 'weight']\n",
      "               │        │     ├── bias: torch.Size([768]) [torch.bfloat16]\n",
      "               │        │     └── weight: torch.Size([768, 3072]) [torch.bfloat16]\n",
      "               │        └──┬ self_attn:\n",
      "               │           ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].self_attn\n",
      "               │           ├─ children: ['k_proj', 'out_proj', 'q_proj', 'v_proj']\n",
      "               │           ├──┬ k_proj\n",
      "               │           |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].self_attn.k_proj\n",
      "               │           |  ├─ children: ['bias', 'weight']\n",
      "               │           │  ├── bias: torch.Size([768]) [torch.bfloat16]\n",
      "               │           │  └── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "               │           ├──┬ out_proj\n",
      "               │           |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].self_attn.out_proj\n",
      "               │           |  ├─ children: ['bias', 'weight']\n",
      "               │           │  ├── bias: torch.Size([768]) [torch.bfloat16]\n",
      "               │           │  └── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "               │           ├──┬ q_proj\n",
      "               │           |  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].self_attn.q_proj\n",
      "               │           |  ├─ children: ['bias', 'weight']\n",
      "               │           │  ├── bias: torch.Size([768]) [torch.bfloat16]\n",
      "               │           │  └── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "               │           └──┬ v_proj:\n",
      "               │              ├─ in: .model.vlm_with_expert.vlm.model.vision_model.encoder.layers.[0..11].self_attn.v_proj\n",
      "               │              ├─ children: ['bias', 'weight']\n",
      "               │              ├── bias: torch.Size([768]) [torch.bfloat16]\n",
      "               │              └── weight: torch.Size([768, 768]) [torch.bfloat16]\n",
      "               └──┬ post_layernorm:\n",
      "                  ├─ in: .model.vlm_with_expert.vlm.model.vision_model.post_layernorm\n",
      "                  ├─ children: ['bias', 'weight']\n",
      "                  ├── bias: torch.Size([768]) [torch.bfloat16]\n",
      "                  └── weight: torch.Size([768]) [torch.bfloat16]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_tree = make_dictionary_tree(state)\n",
    "\n",
    "tree_string = dictionary_tree_as_string(\n",
    "    state_tree,\n",
    ")\n",
    "\n",
    "simple_text = dictionary_tree_as_string(\n",
    "    state_tree,\n",
    "    simplify_numbered_keys=True\n",
    ")\n",
    "\n",
    "with open(f\"{out_dir}/model_shape.txt\", \"w\") as f:\n",
    "    f.write(tree_string)\n",
    "\n",
    "with open(f\"{out_dir}/model_shape_simple.txt\", \"w\") as f:\n",
    "    f.write(simple_text)\n",
    "\n",
    "print(simple_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3250c2",
   "metadata": {},
   "source": [
    "4. Hone in on an attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec02e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_selected_layer(\n",
    "    state_tree,\n",
    "    layer\n",
    "):\n",
    "    keys = layer.split('.')\n",
    "    selected_layer = state_tree\n",
    "    for key in keys:\n",
    "        if key not in selected_layer:\n",
    "            print (f\"Layer {layer} not found in state tree. (stopped at key '{key}')\")\n",
    "            print (\"Available keys at this level:\", list(selected_layer.keys()))\n",
    "            return \n",
    "        \n",
    "        selected_layer = selected_layer[key]\n",
    "\n",
    "    \n",
    "    selected_layer_string = dictionary_tree_as_string(\n",
    "        selected_layer,\n",
    "        key_prefix=layer\n",
    "    )\n",
    "\n",
    "    with open(f\"{out_dir}/selected_layer_{layer.replace('.', '_')}.txt\", \"w\") as f:\n",
    "        f.write(selected_layer_string)\n",
    "\n",
    "layers_to_save = [\n",
    "    'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6',\n",
    "    'model.vlm_with_expert.vlm.model.text_model.layers.3',\n",
    "    'model.vlm_with_expert.lm_expert.layers.2',\n",
    "]\n",
    "\n",
    "for layer in layers_to_save:\n",
    "    save_selected_layer(\n",
    "        state_tree,\n",
    "        layer=layer\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caafdbbb",
   "metadata": {},
   "source": [
    "5. Get the model weights for a given layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121cb58",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41cac220",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shapes = {}\n",
    "\n",
    "for state_name, tensor in state.items():\n",
    "    shapes = tensor.shape\n",
    "\n",
    "    for shape in shapes:\n",
    "        if shape not in state_shapes:\n",
    "            state_shapes[shape] = set()\n",
    "        state_shapes[shape].add(f\"{state_name}: {tensor.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ea8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smolVLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
