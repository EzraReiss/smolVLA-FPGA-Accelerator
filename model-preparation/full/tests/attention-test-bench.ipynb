{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d65bb1cb",
   "metadata": {},
   "source": [
    "# A test bench testing the functional part off a single attention head\n",
    "\n",
    "This gets divided into a few sections\n",
    "\n",
    "1. projection\n",
    "2. QK^T\n",
    "3. softmax(QK^T/sqty(d_h)) = S\n",
    "4. SV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0d1c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d7ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "#Entire attention functional test\n",
    "class TinySelfAttn(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=12):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "\n",
    "    def _shape(self, x, bsz, seq_len):\n",
    "        # (B, S, D) -> (B, num_heads, S, head_dim)\n",
    "        return x.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # x: (B, S, 768)\n",
    "        bsz, seq_len, _ = x.size()\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = self._shape(q, bsz, seq_len)\n",
    "        k = self._shape(k, bsz, seq_len)\n",
    "        v = self._shape(v, bsz, seq_len)\n",
    "\n",
    "        # scaled dot-product\n",
    "        attn_scores = (q @ k.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
    "        if attn_mask is not None:\n",
    "            attn_scores += attn_mask\n",
    "        attn_probs = attn_scores.softmax(dim=-1)\n",
    "\n",
    "        ctx = attn_probs @ v  # (B, heads, S, head_dim)\n",
    "        ctx = ctx.transpose(1, 2).contiguous().view(bsz, seq_len, self.embed_dim)\n",
    "\n",
    "        out = self.out_proj(ctx)\n",
    "        return out\n",
    "\n",
    "# 1) QKV input projection\n",
    "class AttnInputProj(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=12):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "\n",
    "    def _shape(self, x, bsz, seq_len):\n",
    "        # (B, S, D) -> (B, H, S, head_dim)\n",
    "        return x.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, S, D)\n",
    "        bsz, seq_len, _ = x.size()\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = self._shape(q, bsz, seq_len)\n",
    "        k = self._shape(k, bsz, seq_len)\n",
    "        v = self._shape(v, bsz, seq_len)\n",
    "        # all: (B, H, S, head_dim)\n",
    "        return q, k, v\n",
    "\n",
    "\n",
    "# 2) QK^T + scale (+ optional mask)\n",
    "class AttnScores(nn.Module):\n",
    "    def __init__(self, head_dim):\n",
    "        super().__init__()\n",
    "        self.scale = 1.0 / math.sqrt(head_dim)\n",
    "\n",
    "    def forward(self, q, k, attn_mask=None):\n",
    "        # q, k: (B, H, S, head_dim)\n",
    "        # k^T over last two dims\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) * self.scale  # (B, H, S, S)\n",
    "        if attn_mask is not None:\n",
    "            scores = scores + attn_mask  # mask should be broadcastable\n",
    "        return scores\n",
    "\n",
    "\n",
    "# 3) softmax over last dim\n",
    "class AttnWeights(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, scores):\n",
    "        # scores: (B, H, S, S)\n",
    "        return scores.softmax(dim=-1)  # (B, H, S, S)\n",
    "\n",
    "\n",
    "# 4) context = weights @ V\n",
    "class AttnContext(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=12):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "    def forward(self, attn_probs, v):\n",
    "        # attn_probs: (B, H, S, S)\n",
    "        # v:          (B, H, S, head_dim)\n",
    "        ctx = torch.matmul(attn_probs, v)  # (B, H, S, head_dim)\n",
    "        # merge heads back: (B, S, D)\n",
    "        ctx = ctx.transpose(1, 2).contiguous().view(\n",
    "            ctx.size(0),  # B\n",
    "            ctx.size(2),  # S\n",
    "            self.num_heads * self.head_dim  # D\n",
    "        )\n",
    "        return ctx  # (B, S, D)\n",
    "\n",
    "\n",
    "# 5) final output projection\n",
    "class AttnOutputProj(nn.Module):\n",
    "    def __init__(self, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, S, D)\n",
    "        return self.out_proj(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e30fadb",
   "metadata": {},
   "source": [
    "# Load the specific weights for the particular attentio layer we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22de1af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['k_proj.bias', 'k_proj.weight', 'out_proj.bias', 'out_proj.weight', 'q_proj.bias', 'q_proj.weight', 'v_proj.bias', 'v_proj.weight'])\n",
      "k_proj.bias: torch.Size([768]), dtype=torch.bfloat16\n",
      "k_proj.weight: torch.Size([768, 768]), dtype=torch.bfloat16\n",
      "out_proj.bias: torch.Size([768]), dtype=torch.bfloat16\n",
      "out_proj.weight: torch.Size([768, 768]), dtype=torch.bfloat16\n",
      "q_proj.bias: torch.Size([768]), dtype=torch.bfloat16\n",
      "q_proj.weight: torch.Size([768, 768]), dtype=torch.bfloat16\n",
      "v_proj.bias: torch.Size([768]), dtype=torch.bfloat16\n",
      "v_proj.weight: torch.Size([768, 768]), dtype=torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "ALL_WEIGHTS_PATH = \"../weights/downloads/model.safetensors\"\n",
    "state = load_file(ALL_WEIGHTS_PATH)\n",
    "\n",
    "prefix = \"model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn\"\n",
    "\n",
    "# grab only that layerâ€™s params\n",
    "layer_weights = {k[len(prefix)+1:]: v for k, v in state.items() if k.startswith(prefix + \".\")}\n",
    "print(layer_weights.keys())\n",
    "\n",
    "for key in layer_weights:\n",
    "    print(f\"{key}: {layer_weights[key].shape}, dtype={layer_weights[key].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87511ffe",
   "metadata": {},
   "source": [
    "# Below gets the weights for a full attention test (not unit tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b102b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = TinySelfAttn(embed_dim=768, num_heads=12)\n",
    "# make sure dtypes match\n",
    "attn = attn.to(torch.bfloat16)\n",
    "\n",
    "# now load\n",
    "with torch.no_grad():\n",
    "    attn.q_proj.weight.copy_(layer_weights[\"q_proj.weight\"].to(torch.bfloat16))\n",
    "    attn.q_proj.bias.copy_(layer_weights[\"q_proj.bias\"].to(torch.bfloat16))\n",
    "    attn.k_proj.weight.copy_(layer_weights[\"k_proj.weight\"].to(torch.bfloat16))\n",
    "    attn.k_proj.bias.copy_(layer_weights[\"k_proj.bias\"].to(torch.bfloat16))\n",
    "    attn.v_proj.weight.copy_(layer_weights[\"v_proj.weight\"].to(torch.bfloat16))\n",
    "    attn.v_proj.bias.copy_(layer_weights[\"v_proj.bias\"].to(torch.bfloat16))\n",
    "    attn.out_proj.weight.copy_(layer_weights[\"out_proj.weight\"].to(torch.bfloat16))\n",
    "    attn.out_proj.bias.copy_(layer_weights[\"out_proj.bias\"].to(torch.bfloat16))\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2ba4296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "B, S, D = 1, 50, 768  #Batch size, Sequence length, Embedding dimension\n",
    "#Define number of heads\n",
    "num_heads = 12\n",
    "x = torch.randn(B, S, D, dtype=torch.bfloat16)\n",
    "out = attn(x)\n",
    "print(out.shape)  # (1, 50, 768)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35113c",
   "metadata": {},
   "source": [
    "# Below gets just the linear projects of Q, K, and V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc64b05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of x_new: torch.bfloat16\n",
      "Q shape: torch.Size([1, 12, 50, 64]), and type: torch.bfloat16\n",
      "K shape: torch.Size([1, 12, 50, 64]), and type: torch.bfloat16\n",
      "V shape: torch.Size([1, 12, 50, 64]), and type: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "x_new = torch.randn(B, S, D, dtype=torch.bfloat16)\n",
    "print(f\"type of x_new: {x_new.dtype}\")\n",
    "attnInputProj = AttnInputProj(embed_dim=D, num_heads=num_heads).to(torch.bfloat16)\n",
    "#Get the projection weights and biases\n",
    "with torch.no_grad():\n",
    "    attnInputProj.q_proj.weight.copy_(layer_weights[\"q_proj.weight\"].to(torch.bfloat16))\n",
    "    attnInputProj.q_proj.bias.copy_(layer_weights[\"q_proj.bias\"].to(torch.bfloat16))\n",
    "    attnInputProj.k_proj.weight.copy_(layer_weights[\"k_proj.weight\"].to(torch.bfloat16))\n",
    "    attnInputProj.k_proj.bias.copy_(layer_weights[\"k_proj.bias\"].to(torch.bfloat16))\n",
    "    attnInputProj.v_proj.weight.copy_(layer_weights[\"v_proj.weight\"].to(torch.bfloat16))\n",
    "    attnInputProj.v_proj.bias.copy_(layer_weights[\"v_proj.bias\"].to(torch.bfloat16))\n",
    "\n",
    "q, k, v = attnInputProj.forward(x_new)\n",
    "\n",
    "print(f\"Q shape: {q.shape}, and type: {q.dtype}\")\n",
    "print(f\"K shape: {k.shape}, and type: {k.dtype}\")\n",
    "print(f\"V shape: {v.shape}, and type: {v.dtype}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386c809d",
   "metadata": {},
   "source": [
    "# Now we will use this linear projection to calculate QK^T/sqrt(d_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff989dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output scores shape: torch.Size([1, 12, 50, 50]), and type: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "attnScores = AttnScores(head_dim=D//num_heads).to(torch.bfloat16)\n",
    "#create an optional triangular mask filled with negative infinity values\n",
    "mask = torch.tril(torch.ones((S, S), dtype=torch.bfloat16))  # (S, S)\n",
    "mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "output_scores = attnScores.forward(q, k)  #No mask provided\n",
    "print(f\"Output scores shape: {output_scores.shape}, and type: {output_scores.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cb984e",
   "metadata": {},
   "source": [
    "# Test Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e36d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output weights shape: torch.Size([1, 12, 50, 50]), and type: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "AttnWeights = AttnWeights()\n",
    "output_weights = AttnWeights.forward(output_scores)\n",
    "print(f\"Output weights shape: {output_weights.shape}, and type: {output_weights.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9fd79",
   "metadata": {},
   "source": [
    "# Test SV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cef5891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output context shape: torch.Size([1, 50, 768]), and type: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "AttnContext = AttnContext(embed_dim=D, num_heads=num_heads).to(torch.bfloat16)\n",
    "output_context = AttnContext.forward(output_weights, v)\n",
    "print(f\"Output context shape: {output_context.shape}, and type: {output_context.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "168d3f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output final shape: torch.Size([1, 50, 768]), and type: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "AttnOutputProj = AttnOutputProj(embed_dim=D).to(torch.bfloat16)\n",
    "with torch.no_grad():\n",
    "    AttnOutputProj.out_proj.weight.copy_(layer_weights[\"out_proj.weight\"].to(torch.bfloat16))\n",
    "    AttnOutputProj.out_proj.bias.copy_(layer_weights[\"out_proj.bias\"].to(torch.bfloat16))\n",
    "output_final = AttnOutputProj.forward(output_context)\n",
    "\n",
    "#This is the final output of the attention mechanism\n",
    "print(f\"Output final shape: {output_final.shape}, and type: {output_final.dtype}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smolVLA-conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
