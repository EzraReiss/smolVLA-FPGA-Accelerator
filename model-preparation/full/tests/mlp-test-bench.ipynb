{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d65bb1cb",
   "metadata": {},
   "source": [
    "# A test bench testing the functional part of a MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0d1c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d7ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Simple MLP test bench\n",
    "# This notebook now defines a small feed-forward MLP (two-layer MLP with activation)\n",
    "# and runs basic functional tests (shape checks, dtype checks, finite outputs).\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=3072, output_dim=768, activation=None):\n",
    "        super().__init__()\n",
    "        # allow passing None to use a default GELU activation\n",
    "        if activation is None:\n",
    "            activation = nn.GELU()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.act = activation\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, S, D) -> treat last dim as feature dimension\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "# Small helper for printing model summary\n",
    "def model_summary(model, input_shape=None, dtype=torch.float32):\n",
    "    # If input_shape is not provided, infer a reasonable one from model.fc1\n",
    "    if input_shape is None:\n",
    "        try:\n",
    "            input_dim = model.fc1.in_features\n",
    "        except Exception:\n",
    "            input_dim = 768\n",
    "        input_shape = (1, 50, input_dim)\n",
    "    x = torch.randn(*input_shape, dtype=dtype)\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "    print(f\"Input shape: {input_shape}, Output shape: {tuple(out.shape)}, dtype: {out.dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09320e8",
   "metadata": {},
   "source": [
    "# Load the specific weights for the particular MLP we care about\n",
    "\n",
    "This cell will load a safetensors checkpoint (if present) and pick out any parameters related to the action_time MLP, then print the keys and shapes for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39408cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "import os\n",
    "import torch\n",
    "\n",
    "ALL_WEIGHTS_PATH = \"../weights/downloads/model.safetensors\"\n",
    "\n",
    "if not os.path.exists(ALL_WEIGHTS_PATH):\n",
    "    print(f\"Checkpoint not found at {ALL_WEIGHTS_PATH}. If you have a checkpoint, set ALL_WEIGHTS_PATH to its path.\")\n",
    "    state = None\n",
    "else:\n",
    "    state = load_file(ALL_WEIGHTS_PATH)\n",
    "    print(f\"Loaded checkpoint from {ALL_WEIGHTS_PATH} with {len(state)} keys\")\n",
    "\n",
    "# find keys relevant to action_time MLP\n",
    "if state is not None:\n",
    "    action_keys = [k for k in state.keys() if 'action_time_mlp' in k]\n",
    "    print('Found action_time-related keys:', action_keys)\n",
    "    # create a small mapping for easier access\n",
    "    lw = {}\n",
    "    for k in action_keys:\n",
    "        # normalize the end-name so we can match e.g. '...action_time_mlp_in.weight' or '...action_time_mlp_out.weight'\n",
    "        if 'action_time_mlp_in' in k:\n",
    "            short = k.split('action_time_mlp_in')[-1].lstrip('.')\n",
    "            lw['in' + ('.' + short if short else '')] = state[k]\n",
    "        elif 'action_time_mlp_out' in k:\n",
    "            short = k.split('action_time_mlp_out')[-1].lstrip('.')\n",
    "            lw['out' + ('.' + short if short else '')] = state[k]\n",
    "        else:\n",
    "            # fallback: keep full key name\n",
    "            lw[k] = state[k]\n",
    "\n",
    "    print('\\nNormalized keys available:')\n",
    "    for k in lw:\n",
    "        print(k, getattr(lw[k], 'shape', None), getattr(lw[k], 'dtype', None))\n",
    "\n",
    "    # instantiate the SimpleMLP with action_time dims\n",
    "    cfg = {'input_dim': 1440, 'hidden_dim': 720, 'output_dim': 720}\n",
    "    model = SimpleMLP(**cfg)\n",
    "\n",
    "    # attempt to copy into model parameters\n",
    "    with torch.no_grad():\n",
    "        # fc1 weight\n",
    "        possible_in_w = [k for k in state.keys() if k.endswith('action_time_mlp_in.weight') or 'action_time_mlp_in.weight' in k]\n",
    "        possible_in_b = [k for k in state.keys() if k.endswith('action_time_mlp_in.bias') or 'action_time_mlp_in.bias' in k]\n",
    "        possible_out_w = [k for k in state.keys() if k.endswith('action_time_mlp_out.weight') or 'action_time_mlp_out.weight' in k]\n",
    "        possible_out_b = [k for k in state.keys() if k.endswith('action_time_mlp_out.bias') or 'action_time_mlp_out.bias' in k]\n",
    "\n",
    "        def _try_copy(src_key, target_tensor):\n",
    "            t = torch.as_tensor(state[src_key], dtype=torch.float32)\n",
    "            if tuple(t.shape) == tuple(target_tensor.shape):\n",
    "                target_tensor.copy_(t.to(target_tensor.dtype))\n",
    "                print(f'Copied {src_key} -> target shape {t.shape}')\n",
    "                return True\n",
    "            else:\n",
    "                print(f'Shape mismatch {src_key} {t.shape} vs target {tuple(target_tensor.shape)}')\n",
    "                return False\n",
    "\n",
    "        copied = False\n",
    "        if possible_in_w:\n",
    "            copied |= _try_copy(possible_in_w[0], model.fc1.weight)\n",
    "        if possible_in_b:\n",
    "            copied |= _try_copy(possible_in_b[0], model.fc1.bias)\n",
    "        if possible_out_w:\n",
    "            copied |= _try_copy(possible_out_w[0], model.fc2.weight)\n",
    "        if possible_out_b:\n",
    "            copied |= _try_copy(possible_out_b[0], model.fc2.bias)\n",
    "\n",
    "    if not copied:\n",
    "        print('No compatible action_time MLP parameters were copied. You may need to inspect key names or provide a different checkpoint.')\n",
    "\n",
    "    # run a quick forward check and print intermediate shapes/dtypes\n",
    "    x = torch.randn(1, 10, cfg['input_dim'])\n",
    "    with torch.no_grad():\n",
    "        fc1_out = model.fc1(x)\n",
    "        act_out = model.act(fc1_out)\n",
    "        fc2_out = model.fc2(act_out)\n",
    "        final_out = fc2_out\n",
    "\n",
    "    print('\\nForward tensors:')\n",
    "    print('  input:', x.shape, x.dtype)\n",
    "    print('  fc1_out:', fc1_out.shape, fc1_out.dtype)\n",
    "    print('  act_out:', act_out.shape, act_out.dtype)\n",
    "    print('  fc2_out:', fc2_out.shape, fc2_out.dtype)\n",
    "    print('  final_out:', final_out.shape, final_out.dtype)\n",
    "else:\n",
    "    # no checkpoint; still instantiate and run and print intermediates\n",
    "    cfg = {'input_dim': 1440, 'hidden_dim': 720, 'output_dim': 720}\n",
    "    model = SimpleMLP(**cfg)\n",
    "    x = torch.randn(1, 10, cfg['input_dim'])\n",
    "    with torch.no_grad():\n",
    "        fc1_out = model.fc1(x)\n",
    "        act_out = model.act(fc1_out)\n",
    "        fc2_out = model.fc2(act_out)\n",
    "        final_out = fc2_out\n",
    "\n",
    "    print('No checkpoint loaded; sample forward tensors:')\n",
    "    print('  input:', x.shape, x.dtype)\n",
    "    print('  fc1_out:', fc1_out.shape, fc1_out.dtype)\n",
    "    print('  act_out:', act_out.shape, act_out.dtype)\n",
    "    print('  fc2_out:', fc2_out.shape, fc2_out.dtype)\n",
    "    print('  final_out:', final_out.shape, final_out.dtype)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allo-hls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
