{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e889aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os, json, math, re, pathlib, itertools\n",
    "from typing import Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# <<< EDIT IF NEEDED >>>\n",
    "model_weights = \"../weights/downloads/model.safetensors\"  # <= your path\n",
    "out_dir = \"./headers\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c9c75",
   "metadata": {},
   "source": [
    "1) Load safetensors and index keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e8da5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = load_safetensors(model_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158dc433",
   "metadata": {},
   "source": [
    "2. Helper functions for printing out and understanding model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38b5a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dictionary_tree(dict, split='.'):\n",
    "    tree = {}\n",
    "\n",
    "    for key, value in dict.items():\n",
    "        parts = key.split(split)\n",
    "        tree_key = parts[0]\n",
    "        if len(parts) == 1:\n",
    "            tree[tree_key] = value\n",
    "            continue\n",
    "        else:\n",
    "            subtree_key = split.join(parts[1:])\n",
    "            \n",
    "            if tree_key not in tree:\n",
    "                tree[tree_key] = { subtree_key: value }\n",
    "            else:\n",
    "                tree[tree_key][subtree_key] = value\n",
    "\n",
    "    \n",
    "    # if all items are numbers, sort them by numeric order\n",
    "    if all(re.match(r'^\\d+$', str(k)) for k, v in tree.items()):\n",
    "        tree = { k: v for k, v in sorted(tree.items(), key=lambda x: int(x[0])) }\n",
    "\n",
    "    for key in tree.keys():\n",
    "        if isinstance(tree[key], Dict):\n",
    "            tree[key] = make_dictionary_tree(tree[key], split=split)    \n",
    "\n",
    "    return tree\n",
    "\n",
    "def dictionary_tree_as_string(tree, prefix='', key_prefix=''):\n",
    "    items = list(tree.items())\n",
    "    result = \"\"\n",
    "\n",
    "    for key, value in items[:-1]:\n",
    "        if isinstance(value, dict):\n",
    "            result += f\"{prefix}├───┬ {key}\\n\"\n",
    "            if key_prefix:\n",
    "                result += f\"{prefix}|   ├─ in: {key_prefix}.{key}\\n\"\n",
    "            if isinstance(value, dict):\n",
    "                result += f\"{prefix}|   ├─ children: {list(value.keys())}\\n\"\n",
    "            result += dictionary_tree_as_string(\n",
    "                value, \n",
    "                f\"{prefix}│   \",\n",
    "                f\"{key_prefix}.{key}\"\n",
    "            )\n",
    "        else:\n",
    "            result += f\"{prefix}├──── {key}: {value.shape} [{value.dtype}]\\n\"\n",
    "\n",
    "    last_key, last_value = items[-1]\n",
    "    if isinstance(last_value, dict):\n",
    "        result += f\"{prefix}└───┬ {last_key}:\\n\"\n",
    "        if key_prefix:\n",
    "            result += f\"{prefix}    ├─ in: {key_prefix}.{last_key}\\n\"\n",
    "        if isinstance(last_value, dict):\n",
    "            result += f\"{prefix}    ├─ children: {list(last_value.keys())}\\n\"\n",
    "        result += dictionary_tree_as_string(\n",
    "            last_value, \n",
    "            f\"{prefix}    \",\n",
    "            f\"{key_prefix}.{last_key}\"\n",
    "        )\n",
    "    else:\n",
    "        result += f\"{prefix}└──── {last_key}: {last_value.shape} [{last_value.dtype}]\\n\"\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2af4fb",
   "metadata": {},
   "source": [
    "4. Get the different dimensions of the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb8d3a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weight dimensions:\n",
      "  3: 1 tensors\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.embeddings.patch_embedding.weight\n",
      "  16: 1 tensors\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.embeddings.patch_embedding.weight\n",
      "  32: 4 tensors\n",
      "    - model.action_in_proj.weight\n",
      "    - model.state_proj.weight\n",
      "    - model.action_out_proj.bias\n",
      "    - model.action_out_proj.weight\n",
      "  320: 64 tensors\n",
      "    - model.vlm_with_expert.lm_expert.layers.7.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.4.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.10.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.13.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.8.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.15.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.6.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.15.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.9.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.11.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.1.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.0.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.14.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.7.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.10.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.0.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.11.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.3.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.12.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.14.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.2.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.5.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.3.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.1.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.6.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.5.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.2.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.13.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.12.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.4.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.9.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.8.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.v_proj.weight\n",
      "  720: 136 tensors\n",
      "    - model.vlm_with_expert.lm_expert.layers.4.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.11.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.8.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.8.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.0.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.1.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.10.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.6.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.9.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.11.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.5.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.1.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.1.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.0.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.15.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.7.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.2.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.12.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.13.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.3.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.13.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.10.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.5.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.2.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.2.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.5.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.norm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.5.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.10.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.12.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.15.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.8.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.0.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.15.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.12.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.6.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.13.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.5.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.10.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.6.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.2.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.6.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.12.mlp.down_proj.weight\n",
      "    - model.action_in_proj.bias\n",
      "    - model.vlm_with_expert.lm_expert.layers.15.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.14.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.0.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.14.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.10.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.3.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.10.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.3.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.8.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.4.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.13.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.13.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.9.post_attention_layernorm.weight\n",
      "    - model.action_time_mlp_in.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.12.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.0.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.14.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.13.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.2.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.9.self_attn.q_proj.weight\n",
      "    - model.action_time_mlp_out.bias\n",
      "    - model.vlm_with_expert.lm_expert.layers.8.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.11.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.2.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.12.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.8.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.14.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.6.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.6.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.1.mlp.up_proj.weight\n",
      "    - model.action_in_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.11.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.11.self_attn.q_proj.weight\n",
      "    - model.action_time_mlp_out.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.9.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.0.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.4.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.14.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.1.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.7.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.15.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.14.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.3.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.12.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.7.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.15.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.8.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.4.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.2.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.4.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.10.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.9.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.6.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.14.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.8.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.7.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.5.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.4.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.11.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.8.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.11.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.4.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.15.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.14.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.1.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.13.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.12.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.5.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.7.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.0.self_attn.v_proj.weight\n",
      "    - model.action_out_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.7.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.10.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.9.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.1.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.0.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.12.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.3.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.14.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.2.self_attn.v_proj.weight\n",
      "    - model.action_time_mlp_in.bias\n",
      "    - model.vlm_with_expert.lm_expert.layers.3.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.6.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.9.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.2.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.6.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.0.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.3.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.10.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.4.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.7.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.4.mlp.gate_proj.weight\n",
      "  768: 185 tensors\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.v_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.q_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.v_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.k_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.out_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.k_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.q_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.v_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.q_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.q_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.k_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.q_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.out_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.v_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.embeddings.position_embedding.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.v_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.out_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.out_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.out_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.out_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.v_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.v_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.out_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.post_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.layer_norm2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.v_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.embeddings.patch_embedding.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.out_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.k_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.v_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.q_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.out_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.v_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.v_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.layer_norm1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.q_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.layer_norm1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.layer_norm2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.k_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.out_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.q_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.k_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.k_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.k_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.out_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.out_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.out_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.q_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.layer_norm2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.layer_norm2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.post_layernorm.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.layer_norm1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.k_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.out_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.q_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.out_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.out_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.embeddings.patch_embedding.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.out_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.out_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.out_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.layer_norm2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.out_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.out_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.layer_norm2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.out_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.self_attn.k_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.layer_norm2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.k_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.v_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.self_attn.out_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.layer_norm1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.self_attn.k_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.self_attn.out_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.layer_norm1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.self_attn.q_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc2.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.self_attn.q_proj.bias\n",
      "  960: 182 tensors\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.14.input_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.10.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.6.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.10.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.4.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.5.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.1.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.15.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.0.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.7.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.lm_head.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.9.input_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.6.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.13.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.3.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.8.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.5.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.8.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.3.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.0.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.11.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.15.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.embed_tokens.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.5.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.2.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.connector.modality_projection.proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.1.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.7.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.norm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.13.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.4.input_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.12.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.9.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.6.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.2.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.0.input_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.5.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.14.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.11.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.11.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.3.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.12.input_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.6.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.1.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.13.post_attention_layernorm.weight\n",
      "    - model.state_proj.bias\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.10.input_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.15.input_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.8.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.15.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.3.input_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.9.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.14.post_attention_layernorm.weight\n",
      "    - model.state_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.12.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.14.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.2.input_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.13.input_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.2.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.4.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.15.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.10.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.0.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.12.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.4.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.7.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.9.post_attention_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.5.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.2.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.8.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.12.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.1.input_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.9.self_attn.k_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.13.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.14.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.3.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.8.input_layernorm.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.6.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.7.input_layernorm.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.0.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.4.self_attn.q_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.7.self_attn.v_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.10.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.1.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.11.self_attn.o_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.11.post_attention_layernorm.weight\n",
      "  1024: 1 tensors\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.embeddings.position_embedding.weight\n",
      "  1440: 1 tensors\n",
      "    - model.action_time_mlp_in.weight\n",
      "  2048: 48 tensors\n",
      "    - model.vlm_with_expert.lm_expert.layers.4.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.11.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.6.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.13.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.5.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.11.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.6.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.6.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.8.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.9.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.11.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.1.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.12.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.14.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.1.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.13.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.15.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.9.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.1.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.12.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.5.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.10.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.8.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.7.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.12.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.10.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.14.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.15.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.9.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.13.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.0.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.3.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.3.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.0.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.5.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.14.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.2.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.2.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.4.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.2.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.0.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.10.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.15.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.8.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.3.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.7.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.4.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.lm_expert.layers.7.mlp.gate_proj.weight\n",
      "  2560: 48 tensors\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.12.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.6.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.1.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.5.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.2.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.4.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.11.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.8.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.7.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.3.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.9.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.10.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.0.mlp.down_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.15.mlp.gate_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.14.mlp.up_proj.weight\n",
      "    - model.vlm_with_expert.vlm.model.text_model.layers.13.mlp.gate_proj.weight\n",
      "  3072: 36 tensors\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.11.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.1.mlp.fc1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.0.mlp.fc1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.10.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.7.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.5.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.3.mlp.fc2.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.8.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.9.mlp.fc1.weight\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.2.mlp.fc1.bias\n",
      "    - model.vlm_with_expert.vlm.model.vision_model.encoder.layers.4.mlp.fc2.weight\n",
      "  12288: 1 tensors\n",
      "    - model.vlm_with_expert.vlm.model.connector.modality_projection.proj.weight\n",
      "  49280: 2 tensors\n",
      "    - model.vlm_with_expert.vlm.model.text_model.embed_tokens.weight\n",
      "    - model.vlm_with_expert.vlm.lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "dimensions = {}\n",
    "\n",
    "for name, param in state.items():\n",
    "    for shape in param.shape:\n",
    "        if shape not in dimensions:\n",
    "            dimensions[shape] = set()\n",
    "        dimensions[shape].add(name)\n",
    "        \n",
    "print(\"Model weight dimensions:\")\n",
    "for dim, names in sorted(dimensions.items()):\n",
    "    print(f\"  {dim}: {len(names)} tensors\")\n",
    "    for n in list(names):\n",
    "        print(f\"    - {n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "838ac802",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CppCode:\n",
    "    def __init__(self):\n",
    "        self.pragmas = []\n",
    "        self.imports: list[str] = []\n",
    "        self.lines: list[str] = []\n",
    "        self.namespaces: dict[str, CppCode] = {}\n",
    "\n",
    "    def add_pragma(self, pragma: str):\n",
    "        if pragma not in self.pragmas:\n",
    "            self.pragmas.append(pragma)\n",
    "\n",
    "    def get_namespace(\n",
    "        self, \n",
    "        namespace: list[str] | str\n",
    "    ) -> 'CppCode':\n",
    "        if isinstance(namespace, str):\n",
    "            namespace = [namespace]\n",
    "            \n",
    "        if namespace:\n",
    "            name = namespace[0]\n",
    "\n",
    "            # if name is just a number, prefix with underscore\n",
    "            if re.match(r'^\\d+$', name):\n",
    "                name = f\"_{name}\"\n",
    "\n",
    "            if name not in self.namespaces:\n",
    "                self.namespaces[name] = CppCode()\n",
    "            \n",
    "            subspace = self.namespaces[name]\n",
    "            \n",
    "            return subspace.get_namespace(\n",
    "                namespace=namespace[1:]\n",
    "            )\n",
    "        else:\n",
    "            return self\n",
    "\n",
    "    def add_import(self, import_line: str):\n",
    "        for line in import_line.splitlines():\n",
    "            self.imports.append(line.strip())\n",
    "\n",
    "    def add_line(self, line: str):\n",
    "        for line in line.splitlines():\n",
    "            self.lines.append(line.strip())\n",
    "            \n",
    "    def write_string(\n",
    "        self,\n",
    "        indent: str = '',\n",
    "    ) -> str:\n",
    "        result = \"\"\n",
    "        if self.pragmas:\n",
    "            for pragma in self.pragmas:\n",
    "                result += f\"{indent}{pragma}\\n\"\n",
    "            result += \"\\n\"\n",
    "        if self.imports:\n",
    "            for imp in self.imports:\n",
    "                result += f\"{indent}{imp}\\n\"\n",
    "            result += \"\\n\"\n",
    "        if self.lines:\n",
    "            for line in self.lines:\n",
    "                result += f\"{indent}{line}\\n\"\n",
    "        for name, subspace in self.namespaces.items():\n",
    "            result += f\"{indent}namespace {name} {{\\n\"\n",
    "            result += subspace.write_string(indent + '    ')\n",
    "            result += f\"{indent}}} // namespace {name}\\n\\n\"\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93320d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = CppCode()\n",
    "\n",
    "header.add_pragma(\"#pragma once\")\n",
    "header.add_import(\"#include <cstdint>\")\n",
    "\n",
    "smolvla = header.get_namespace('smolvla')\n",
    "\n",
    "smolvla.add_line(\"\"\"\n",
    "// === Type Definitions ===\n",
    "using size_t = std::size_t; // Standard size type (for shape)\n",
    "\n",
    "using float32_t = float;    // 32-bit floating point\n",
    "using bfloat16_t = float;   // 16-bit bfloat16 floating point (TODO: implement proper bfloat16 type)\n",
    "\"\"\")\n",
    "\n",
    "size_to_constant_name = {\n",
    "    3: \"PATCH_EMBED_DIM_SMALL\",\n",
    "    16: \"PATCH_EMBED_DIM_LARGE\",\n",
    "    768: \"VISION_HIDDEN_DIM\",\n",
    "    3072: \"VISION_FFN_DIM\",\n",
    "    1024: \"VISION_POS_EMBED_DIM\",\n",
    "    12288: \"VISION_CONNECTOR_DIM\",\n",
    "    32: \"ACTION_STATE_DIM\",\n",
    "    1440: \"ACTION_TIME_MLP_IN_DIM\",\n",
    "    720: \"HIDDEN_DIM\",\n",
    "    960: \"TEXT_EMBED_DIM\",\n",
    "    2560: \"TEXT_FFN_DIM\",\n",
    "    2048: \"EXPERT_FFN_DIM\",\n",
    "    320: \"ATTENTION_PROJ_DIM\",\n",
    "    49280: \"TOKEN_EMBED_DIM\",\n",
    "    12208: \"CONNECTOR_PROJ_DIM\",\n",
    "}\n",
    "\n",
    "smolvla.add_line(\"\"\"\n",
    "// === Model Dimension Constants ===\n",
    "\"\"\")\n",
    "for dim, name in sorted(size_to_constant_name.items()):\n",
    "    smolvla.add_line(f\"constexpr size_t {name} = {dim};\")\n",
    "\n",
    "smolvla.add_line(\"\"\"\n",
    "// === Model Weight Declarations ===\n",
    "\"\"\")\n",
    "\n",
    "for name, param in state.items():\n",
    "    param_array = name.split('.')\n",
    "\n",
    "    namespace = smolvla.get_namespace(param_array[:-1])\n",
    "    var_name = param_array[-1]\n",
    "    \n",
    "    if param.dtype == torch.float32:\n",
    "        dtype_str = \"float32_t\"\n",
    "    elif param.dtype == torch.bfloat16:\n",
    "        dtype_str = \"bfloat16_t\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype: {param.dtype} for {name}\")\n",
    "\n",
    "    shape_str = f\"[{']['.join(size_to_constant_name[s] for s in param.shape)}]\"\n",
    "    namespace.add_line(f\"const {dtype_str} {var_name}{shape_str};\")\n",
    "\n",
    "header_text = header.write_string()\n",
    "with open(os.path.join(out_dir, \"model_weights.h\"), 'w') as f:\n",
    "    f.write(header_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a568c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cpp_code_structure(\n",
    "    state_tree,\n",
    "    namespace: CppCode\n",
    "):\n",
    "    for key, value in state_tree.items():\n",
    "        if isinstance(value, dict):\n",
    "            subspace = namespace.get_namespace(key)\n",
    "            create_cpp_code_structure(\n",
    "                state_tree=value,\n",
    "                namespace=subspace\n",
    "            )\n",
    "        else:\n",
    "            if value.dtype == torch.float32:\n",
    "                dtype_str = \"float32_t\"\n",
    "            elif value.dtype == torch.bfloat16:\n",
    "                dtype_str = \"bfloat16_t\"\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported dtype: {value.dtype} for {key}\")\n",
    "\n",
    "            shape_str = f\"[{']['.join(size_to_constant_name[s] for s in value.shape)}]\"\n",
    "            namespace.add_line(f\"const {dtype_str} {key}{shape_str} = {{\")\n",
    "            # add values after converting to float32 to numpy\n",
    "            values = value.to(torch.float32).cpu().numpy()\n",
    "            # if its 2 dimensions, format as matrix\n",
    "            if len(values.shape) == 2:\n",
    "                for row in values:\n",
    "                    row_str = ', '.join(f\"{v}\" for v in row)\n",
    "                    namespace.add_line(f\"    {{ {row_str} }},\")\n",
    "            else:\n",
    "                flat_values = values.flatten()\n",
    "                line = \"    \"\n",
    "                for i, v in enumerate(flat_values):\n",
    "                    line += f\"{v}, \"\n",
    "                    if (i + 1) % 10 == 0:\n",
    "                        namespace.add_line(line)\n",
    "                        line = \"    \"\n",
    "                if line.strip():\n",
    "                    namespace.add_line(line)\n",
    "            \n",
    "            namespace.add_line(\"};\\n\") \n",
    "        \n",
    "\n",
    "def save_selected_layer(\n",
    "    state_tree,\n",
    "    layer\n",
    "):\n",
    "    source = CppCode()\n",
    "    source.add_import('#include \"model_weights.h\"')\n",
    "    \n",
    "    keys = layer.split('.')\n",
    "    namespace = source\n",
    "    selected_layer = state_tree\n",
    "    for key in keys:\n",
    "        if key not in selected_layer:\n",
    "            print (f\"Layer {layer} not found in state tree. (stopped at key '{key}')\")\n",
    "            print (\"Available keys at this level:\", list(selected_layer.keys()))\n",
    "            return \n",
    "        \n",
    "        namespace = namespace.get_namespace(key)\n",
    "        selected_layer = selected_layer[key]\n",
    "\n",
    "    create_cpp_code_structure(\n",
    "        state_tree=selected_layer,\n",
    "        namespace=namespace\n",
    "    )\n",
    "    source_text = source.write_string()\n",
    "    layer_filename = \"WEIGHTS_\" + layer.replace('.', '_') + \".cpp\"\n",
    "    with open(os.path.join(out_dir, layer_filename), 'w') as f:\n",
    "        f.write(source_text)\n",
    "\n",
    "layers_to_save = [\n",
    "    'model.vlm_with_expert.vlm.model.vision_model.encoder.layers.6',\n",
    "    'model.vlm_with_expert.vlm.model.text_model.layers.3',\n",
    "    'model.vlm_with_expert.lm_expert.layers.2',\n",
    "]\n",
    "\n",
    "state_tree = make_dictionary_tree(state)\n",
    "\n",
    "for layer in layers_to_save:\n",
    "    save_selected_layer(\n",
    "        state_tree,\n",
    "        layer=layer\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smolVLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
